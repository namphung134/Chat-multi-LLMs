{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namphung134/Multi_llm_chatbot/blob/main/Web%2Byoutube_crawl%2Bsummary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_3kVsdupi2_"
      },
      "source": [
        "# **Install requirement libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g2qKzKnhpda6",
        "outputId": "2e8f8511-15c6-4ad1-ae12-4d9cc8505f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting obsei\n",
            "  Downloading obsei-0.0.15-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.9.3 in /usr/local/lib/python3.11/dist-packages (from obsei) (4.13.3)\n",
            "Collecting dateparser>=1.2.0 (from obsei)\n",
            "  Downloading dateparser-1.2.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting mmh3>=4.0.1 (from obsei)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting pydantic-settings>=2.1.0 (from obsei)\n",
            "  Downloading pydantic_settings-2.8.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from obsei) (2.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from obsei) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2023.3.post1 in /usr/local/lib/python3.11/dist-packages (from obsei) (2025.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from obsei) (2.32.3)\n",
            "Requirement already satisfied: sqlalchemy>=2.0.24 in /usr/local/lib/python3.11/dist-packages (from obsei) (2.0.38)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9.3->obsei) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9.3->obsei) (4.12.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.2.0->obsei) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.2.0->obsei) (5.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.3->obsei) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.5.3->obsei) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.1.0->obsei)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->obsei) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->obsei) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->obsei) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->obsei) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->obsei) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=2.0.24->obsei) (3.1.1)\n",
            "Downloading obsei-0.0.15-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.2.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.0-py3-none-any.whl (30 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, mmh3, dateparser, pydantic-settings, obsei\n",
            "Successfully installed dateparser-1.2.1 mmh3-5.1.0 obsei-0.0.15 pydantic-settings-2.8.0 python-dotenv-1.0.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2025.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting trafilatura\n",
            "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2025.1.31)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (3.4.1)\n",
            "Collecting courlan>=1.3.2 (from trafilatura)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.9.2 (from trafilatura)\n",
            "  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura)\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (5.3.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from trafilatura) (2.3.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from htmldate>=1.9.2->trafilatura) (1.2.1)\n",
            "Collecting python-dateutil>=2.9.0.post0 (from htmldate>=1.9.2->trafilatura)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n",
            "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
            "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: tld, python-dateutil, lxml_html_clean, courlan, justext, htmldate, trafilatura\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.2\n",
            "    Uninstalling python-dateutil-2.8.2:\n",
            "      Successfully uninstalled python-dateutil-2.8.2\n",
            "Successfully installed courlan-1.3.2 htmldate-1.9.3 justext-3.0.2 lxml_html_clean-0.4.1 python-dateutil-2.9.0.post0 tld-0.13 trafilatura-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              },
              "id": "fcf9197990324c64b6aaabccdd174f05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=78ea22dee92caa0bbe7875fdd7b36081fedce5159d9a67ff30c3d815c86d5090\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install obsei\n",
        "!pip install transformers\n",
        "!pip install dateparser\n",
        "!pip install sentencepiece\n",
        "!pip install trafilatura\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ho0SFdEp0XW"
      },
      "source": [
        "# Config google news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NusmBJylp400"
      },
      "outputs": [],
      "source": [
        "import dateparser\n",
        "from gnews import GNews\n",
        "from pydantic import PrivateAttr\n",
        "from datetime import datetime, date, timedelta, time, timezone\n",
        "from typing import Any, Dict, List, Optional\n",
        "from urllib import parse\n",
        "from obsei.payload import TextPayload\n",
        "from obsei.misc.utils import DATETIME_STRING_PATTERN, convert_utc_time\n",
        "from obsei.source.base_source import BaseSource, BaseSourceConfig\n",
        "from obsei.source.website_crawler_source import (\n",
        "    BaseCrawlerConfig,\n",
        "    TrafilaturaCrawlerConfig,\n",
        ")\n",
        "\n",
        "GOOGLE_DATE_TIME_QUERY_PATTERN = \"%Y-%m-%d\"\n",
        "\n",
        "class GoogleNewsConfig(BaseSourceConfig):\n",
        "    _google_news_client: GNews = PrivateAttr()\n",
        "    TYPE: str = \"GoogleNews\"\n",
        "    query: str\n",
        "    country: Optional[str] = \"US\"\n",
        "    language: Optional[str] = \"en\"\n",
        "    max_results: Optional[int] = 100\n",
        "    lookup_period: Optional[str] = None\n",
        "    after_date: Optional[str] = None\n",
        "    before_date: Optional[str] = None\n",
        "    fetch_article: Optional[bool] = True\n",
        "    crawler_config: Optional[BaseCrawlerConfig] = None\n",
        "\n",
        "    def __init__(self, **data: Any):\n",
        "        super().__init__(**data)\n",
        "\n",
        "        if self.lookup_period and self.after_date:\n",
        "            raise AttributeError(\"Can't use `lookup_period` and `after_date` both\")\n",
        "        elif not self.after_date and self.before_date:\n",
        "            raise AttributeError(\"Can't use `before_date` without `after_date` or `lookup_period`\")\n",
        "\n",
        "        if self.lookup_period:\n",
        "            after_time = convert_utc_time(self.lookup_period)\n",
        "            self.after_date = after_time.strftime(GOOGLE_DATE_TIME_QUERY_PATTERN)\n",
        "\n",
        "        if not self.before_date:\n",
        "            before_time = datetime.combine(date.today(), time(tzinfo=timezone.utc)) + timedelta(days=1)\n",
        "            self.before_date = before_time.strftime(GOOGLE_DATE_TIME_QUERY_PATTERN)\n",
        "\n",
        "        self._google_news_client = GNews(\n",
        "            language=self.language,\n",
        "            country=self.country,\n",
        "            max_results=self.max_results,\n",
        "        )\n",
        "\n",
        "        if not self.crawler_config:\n",
        "            self.crawler_config = TrafilaturaCrawlerConfig(urls=[])\n",
        "\n",
        "    def get_client(self) -> GNews:\n",
        "        return self._google_news_client\n",
        "\n",
        "\n",
        "class GoogleNewsSource(BaseSource):\n",
        "    NAME: Optional[str] = \"GoogleNews\"\n",
        "\n",
        "    def lookup(self, config: GoogleNewsConfig, **kwargs) -> List[TextPayload]:  # type: ignore[override]\n",
        "        source_responses: List[TextPayload] = []\n",
        "\n",
        "        # Get data from state\n",
        "        id: str = kwargs.get(\"id\", None)\n",
        "        state: Optional[Dict[str, Any]] = (\n",
        "            None\n",
        "            if id is None or self.store is None\n",
        "            else self.store.get_source_state(id)\n",
        "        )\n",
        "        update_state: bool = True if id else False\n",
        "        state = state or dict()\n",
        "        lookup_period: str = state.get(\"since_time\", None)\n",
        "        since_time = None if not lookup_period else convert_utc_time(lookup_period)\n",
        "        last_since_time = since_time\n",
        "\n",
        "        last_after_time = convert_utc_time(config.after_date) if config.after_date else None\n",
        "        if since_time and last_after_time:\n",
        "            last_after_time = since_time if since_time > last_after_time else last_since_time\n",
        "        elif not last_after_time:\n",
        "            last_after_time = datetime.combine(date.today(), time(tzinfo=timezone.utc))\n",
        "\n",
        "        before_time = convert_utc_time(config.before_date) if config.after_date else None\n",
        "        if not before_time or before_time > datetime.combine(date.today(), time(tzinfo=timezone.utc)):\n",
        "            before_time = datetime.combine(date.today(), time(tzinfo=timezone.utc)) + timedelta(days=1)\n",
        "\n",
        "        google_news_client = config.get_client()\n",
        "        more_data_exist = True\n",
        "        while more_data_exist and before_time > last_after_time:\n",
        "            after_time = before_time - timedelta(days=1)\n",
        "            after_date = after_time.strftime(GOOGLE_DATE_TIME_QUERY_PATTERN)\n",
        "            before_date = before_time.strftime(GOOGLE_DATE_TIME_QUERY_PATTERN)\n",
        "\n",
        "            new_query = f'{config.query}+after:{after_date}+before:{before_date}'\n",
        "            query = parse.quote(new_query, errors='ignore')\n",
        "\n",
        "            before_time = after_time\n",
        "\n",
        "            articles = google_news_client.get_news(query)\n",
        "\n",
        "            for article in articles:\n",
        "                published_date = (\n",
        "                    None\n",
        "                    if article[\"published date\"] == \"\"\n",
        "                    else dateparser.parse(article[\"published date\"])\n",
        "                )\n",
        "\n",
        "                if config.fetch_article and config.crawler_config:\n",
        "                    extracted_data = config.crawler_config.extract_url(url=article[\"url\"])\n",
        "\n",
        "                    if extracted_data is not None and extracted_data.get(\"text\", None) is not None:\n",
        "                        article_text = extracted_data[\"text\"]\n",
        "                        del extracted_data[\"text\"]\n",
        "                    else:\n",
        "                        article_text = \"\"\n",
        "\n",
        "                    article[\"extracted_data\"] = extracted_data\n",
        "                else:\n",
        "                    article_text = article[\"description\"]\n",
        "\n",
        "                source_responses.append(\n",
        "                    TextPayload(\n",
        "                        processed_text=f\"{article['title']}.\\n\\n {article_text}\",\n",
        "                        meta=vars(article) if hasattr(article, \"__dict__\") else article,\n",
        "                        source_name=self.NAME,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                if config.max_results is not None and len(source_responses) >= config.max_results:\n",
        "                    source_responses = source_responses[:config.max_results]\n",
        "                    more_data_exist = False\n",
        "                    break\n",
        "\n",
        "                if published_date and since_time and published_date < since_time:\n",
        "                    more_data_exist = False\n",
        "                    break\n",
        "                if last_since_time is None or (\n",
        "                    published_date and last_since_time < published_date\n",
        "                ):\n",
        "                    last_since_time = published_date\n",
        "\n",
        "            if update_state and last_since_time and self.store is not None:\n",
        "                state[\"since_time\"] = last_since_time.strftime(DATETIME_STRING_PATTERN)\n",
        "                self.store.update_source_state(workflow_id=id, state=state)\n",
        "\n",
        "        return source_responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8yTJCWUuSNM"
      },
      "source": [
        "# Crawl gg news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIexKGDeuR5U",
        "outputId": "08e72151-7dad-4bb5-9b66-12074a293906"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:trafilatura.xml:missing link attribute: None {'target': '/lich-thi-dau/lich-thi-daucau-longsea-games-31-hom-nay-vtv6-vtv5-truc-tiep-n20220517085553236.htm'}\n",
            "ERROR:trafilatura.xml:empty link: None {'target': '/lich-thi-dau/lich-thi-daucau-longsea-games-31-hom-nay-vtv6-vtv5-truc-tiep-n20220517085553236.htm'}\n",
            "WARNING:trafilatura.xml:missing link attribute: None {'target': '/bong-da-viet/ket-quabong-da-sea-games-31ket-qua-bong-dau23-viet-nam-moi-nhat-n20220514171621213.htm'}\n",
            "ERROR:trafilatura.xml:empty link: None {'target': '/bong-da-viet/ket-quabong-da-sea-games-31ket-qua-bong-dau23-viet-nam-moi-nhat-n20220514171621213.htm'}\n",
            "WARNING:trafilatura.xml:missing link attribute: None {'target': '/sea-games-31/truc-tiep-sea-games-31-hom-nay-215-nu-viet-nam-tranh-hcv-voi-thai-lan-n20220521113223135.htm'}\n",
            "ERROR:trafilatura.xml:empty link: None {'target': '/sea-games-31/truc-tiep-sea-games-31-hom-nay-215-nu-viet-nam-tranh-hcv-voi-thai-lan-n20220521113223135.htm'}\n",
            "WARNING:trafilatura.xml:missing link attribute: None {'target': '/sea-games-31/truc-tiep-sea-games-145-vtv6-vtv5-dien-kinh-boi-loi-bong-chuyen-esports-wushu-n20220513203805963.htm'}\n",
            "ERROR:trafilatura.xml:empty link: None {'target': '/sea-games-31/truc-tiep-sea-games-145-vtv6-vtv5-dien-kinh-boi-loi-bong-chuyen-esports-wushu-n20220513203805963.htm'}\n",
            "WARNING:trafilatura.xml:missing link attribute: None {'target': '/truc-tiep-bong-da/vtv6-truc-tiep-bong-da-nu-viet-nam-vs-thai-lan-chung-ket-sea-games-31-19h00-215-n20220520115639702.htm'}\n",
            "ERROR:trafilatura.xml:empty link: None {'target': '/truc-tiep-bong-da/vtv6-truc-tiep-bong-da-nu-viet-nam-vs-thai-lan-chung-ket-sea-games-31-19h00-215-n20220520115639702.htm'}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'title': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5)', 'author': 'PV; BTV-TTVH', 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-19', 'categories': 'Thể thao', 'tags': 'Lich thi dau Sea games 31, lich thi dau sea games, trực tiếp cầu lông Sea games 31, xem trực tiếp cầu lông sea games, lich thi dau cau long, trực tiếp cầu lông việt nam;Lich thi dau Sea games 31;lich thi dau sea games;trực tiếp cầu lông Sea games 31;xem trực tiếp cầu lông sea games;lich thi dau cau long;trực tiếp cầu lông việt nam', 'fingerprint': 'j+u6ya16H0vdxcOe4epzsKjBhXE=', 'id': '91ec24fb', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5) Thứ Năm, 19/05/2022 10:55 GMT+7 Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31. Lịch thi đấu cầu lông SEA Games 31 - Xem VTV6 VTV5 trực tiếp cầu lông SEA Games 31 năm 2022. Lịch thi đấu môn cầu lông Việt Nam SEA Games 31. Xem trực tiếp cầu lông SEA Games Lịch thi đấu cầu lông SEA Games 31: Ngày 18/5: 13h00 - 16h00: Đồng đội nữ 18h00 - 21h00: Đồng đội nam Ngày 19/5: 9h00 - 21h00: Vòng 1/16 đơn nam Vòng 1/16 đơn nữ Vòng 1/16 đôi nam Vòng 1/16 đôi nữ Vòng 1/16 đôi nam nữ Ngày 20/5: 9h00 - 17h00: Tứ kết đơn nam Tứ kết đơn nữ Tứ kết đôi nam Tứ kết đôi nữ Tứ kết đôi nam nữ Ngày 21/5: 13h00 - 18h00 Bán kết đơn nam Bán kết đơn nữ Bán kết đôi nam Bán kết đôi nữ Bán kết đôi nam nữ Ngày 22/5: Từ 12h00: Chung kết đơn nam Trao huy chương đơn nam Chung kết đơn nữ Trao huy chương đơn nữ Chung kết đôi nam Trao huy chương đôi nam Chung kết đôi nữ Trao huy chương đôi nữ Chung kết đôi nam nữ Trao huy chương đôi nam nữ Cầu lông SEA Games 31 diễn ra khi nào, ở đâu? Cầu lông SEA Games 31 diễn ra từ ngày 16/5 đến ngày 22/5 tại Nhà thi đấu thể thao Bắc Giang. Các nội dung Cầu lông SEA Games 2022 Môn Cầu lông gồm 7 nội dung thi đấu: 3 nội dung nam, 3 nội dung nữ và 1 nội dung đôi nam, nữ Nam: Đơn, Đôi, Đồng đội Nữ: Đơn, Đôi, Đồng đội Phối hợp: Đôi nam nữ Danh sách đội tuyển Cầu lông Việt Nam tại SEA Games 2022 Mỗi đoàn thể thao (NOC) được đăng ký tối đa 10 vận động viên nam và 10 vận động viên nữ ở môn Cầu lông. Nội dung đơn: Mỗi NOC được cử tối đa 2) vận động viên nam và 2 vận động viên nữ. Nội dung đôi: Mỗi NOC được cử tối đa 2 đôi cho mỗi nội dung đôi trong 3 nội dung thi đấu đôi. T.G', 'source': 'https://thethaovanhoa.vn/news-20220516095325404.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31.'}\n",
            "{'title': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5)', 'author': 'PV; BTV-TTVH', 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-19', 'categories': 'Thể thao', 'tags': 'Lich thi dau Sea games 31, lich thi dau sea games, trực tiếp cầu lông Sea games 31, xem trực tiếp cầu lông sea games, lich thi dau cau long, trực tiếp cầu lông việt nam;Lich thi dau Sea games 31;lich thi dau sea games;trực tiếp cầu lông Sea games 31;xem trực tiếp cầu lông sea games;lich thi dau cau long;trực tiếp cầu lông việt nam', 'fingerprint': 'j+u6ya16H0vdxcOe4epzsKjBhXE=', 'id': '91ec24fb', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5) Thứ Năm, 19/05/2022 10:55 GMT+7 Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31. Lịch thi đấu cầu lông SEA Games 31 - Xem VTV6 VTV5 trực tiếp cầu lông SEA Games 31 năm 2022. Lịch thi đấu môn cầu lông Việt Nam SEA Games 31. Xem trực tiếp cầu lông SEA Games Lịch thi đấu cầu lông SEA Games 31: Ngày 18/5: 13h00 - 16h00: Đồng đội nữ 18h00 - 21h00: Đồng đội nam Ngày 19/5: 9h00 - 21h00: Vòng 1/16 đơn nam Vòng 1/16 đơn nữ Vòng 1/16 đôi nam Vòng 1/16 đôi nữ Vòng 1/16 đôi nam nữ Ngày 20/5: 9h00 - 17h00: Tứ kết đơn nam Tứ kết đơn nữ Tứ kết đôi nam Tứ kết đôi nữ Tứ kết đôi nam nữ Ngày 21/5: 13h00 - 18h00 Bán kết đơn nam Bán kết đơn nữ Bán kết đôi nam Bán kết đôi nữ Bán kết đôi nam nữ Ngày 22/5: Từ 12h00: Chung kết đơn nam Trao huy chương đơn nam Chung kết đơn nữ Trao huy chương đơn nữ Chung kết đôi nam Trao huy chương đôi nam Chung kết đôi nữ Trao huy chương đôi nữ Chung kết đôi nam nữ Trao huy chương đôi nam nữ Cầu lông SEA Games 31 diễn ra khi nào, ở đâu? Cầu lông SEA Games 31 diễn ra từ ngày 16/5 đến ngày 22/5 tại Nhà thi đấu thể thao Bắc Giang. Các nội dung Cầu lông SEA Games 2022 Môn Cầu lông gồm 7 nội dung thi đấu: 3 nội dung nam, 3 nội dung nữ và 1 nội dung đôi nam, nữ Nam: Đơn, Đôi, Đồng đội Nữ: Đơn, Đôi, Đồng đội Phối hợp: Đôi nam nữ Danh sách đội tuyển Cầu lông Việt Nam tại SEA Games 2022 Mỗi đoàn thể thao (NOC) được đăng ký tối đa 10 vận động viên nam và 10 vận động viên nữ ở môn Cầu lông. Nội dung đơn: Mỗi NOC được cử tối đa 2) vận động viên nam và 2 vận động viên nữ. Nội dung đôi: Mỗi NOC được cử tối đa 2 đôi cho mỗi nội dung đôi trong 3 nội dung thi đấu đôi. T.G', 'source': 'https://thethaovanhoa.vn/news-20220516095325404.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31.'}\n",
            "{'title': 'Lịch thi đấu tennis SEA Games 31 mới nhất', 'author': None, 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-18', 'categories': 'Thể thao', 'tags': 'Lịch thi đấu tennis SEA Games 31, VTV6 trực tiếp quần vợt, SEA Games 31, tennis, quần vợt, tennis SEA Games 31, quần vợt SEA Games 31, lich thi dau tennis, VTV6;Lịch thi đấu tennis SEA Games 31;VTV6 trực tiếp quần vợt;SEA Games 31;tennis;quần vợt;tennis SEA Games 31;quần vợt SEA Games 31;lich thi dau tennis;VTV6', 'fingerprint': 'ICHEeqL4EwYTsXOW5i/f2oj1vj4=', 'id': '4252224d', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay. Kết quả bóng đá SEA Games 31. Kết quả bóng đá U23 Việt Nam mới nhất. Kết quả bóng đá SEA Games 2022. Kết quả bóng đá SEA Games hôm nay. Lịch thi đấu tennis SEA Games 31 ngày 17/5: * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) Lịch thi đấu tennis SEA Games 31: * 10h ngày 13/5: Vòng 1 nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 14/5: Bán kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 15/5: Chung kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 18/5: Tứ kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 19/5: Vòng 1 nội dung đơn nam/đơn nữ; Bán kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 20/5: Tứ kết nội dung đơn nam/đơn nữ; Chung kết đôi nam nữ (VTV6, VTV5) * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) * 10h ngày 22/5: Chung kết nội dung đơn nam/đơn nữ (VTV6, VTV5) Tại SEA Games 31, nội dung tennis sẽ diễn ra từ 13-22/5/2022 ở cụm sân quần vợt Hanaka Paris Ocean Park thuộc thành phố Từ Sơn, tỉnh Bắc Ninh. Dự kiến các nội dung đồng đội sẽ đấu từ ngày 13-15/5 trước lúc tranh huy chương vào ngày 16/5. Từ 17-20/5 là thời gian dành cho các vòng đấu đôi và đơn, trước lúc tranh huy chương vào các ngày 21-22/5. Tay vợt nam số 1 Việt Nam Lý Hoàng Nam sẽ bảo vệ danh hiệu vô địch giành được ở SEA Games 30 tại Philippines. V.M', 'source': 'https://thethaovanhoa.vn/news-20220516210251757.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay.'}\n",
            "{'title': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5)', 'author': 'PV; BTV-TTVH', 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-19', 'categories': 'Thể thao', 'tags': 'Lich thi dau Sea games 31, lich thi dau sea games, trực tiếp cầu lông Sea games 31, xem trực tiếp cầu lông sea games, lich thi dau cau long, trực tiếp cầu lông việt nam;Lich thi dau Sea games 31;lich thi dau sea games;trực tiếp cầu lông Sea games 31;xem trực tiếp cầu lông sea games;lich thi dau cau long;trực tiếp cầu lông việt nam', 'fingerprint': 'j+u6ya16H0vdxcOe4epzsKjBhXE=', 'id': '91ec24fb', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5) Thứ Năm, 19/05/2022 10:55 GMT+7 Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31. Lịch thi đấu cầu lông SEA Games 31 - Xem VTV6 VTV5 trực tiếp cầu lông SEA Games 31 năm 2022. Lịch thi đấu môn cầu lông Việt Nam SEA Games 31. Xem trực tiếp cầu lông SEA Games Lịch thi đấu cầu lông SEA Games 31: Ngày 18/5: 13h00 - 16h00: Đồng đội nữ 18h00 - 21h00: Đồng đội nam Ngày 19/5: 9h00 - 21h00: Vòng 1/16 đơn nam Vòng 1/16 đơn nữ Vòng 1/16 đôi nam Vòng 1/16 đôi nữ Vòng 1/16 đôi nam nữ Ngày 20/5: 9h00 - 17h00: Tứ kết đơn nam Tứ kết đơn nữ Tứ kết đôi nam Tứ kết đôi nữ Tứ kết đôi nam nữ Ngày 21/5: 13h00 - 18h00 Bán kết đơn nam Bán kết đơn nữ Bán kết đôi nam Bán kết đôi nữ Bán kết đôi nam nữ Ngày 22/5: Từ 12h00: Chung kết đơn nam Trao huy chương đơn nam Chung kết đơn nữ Trao huy chương đơn nữ Chung kết đôi nam Trao huy chương đôi nam Chung kết đôi nữ Trao huy chương đôi nữ Chung kết đôi nam nữ Trao huy chương đôi nam nữ Cầu lông SEA Games 31 diễn ra khi nào, ở đâu? Cầu lông SEA Games 31 diễn ra từ ngày 16/5 đến ngày 22/5 tại Nhà thi đấu thể thao Bắc Giang. Các nội dung Cầu lông SEA Games 2022 Môn Cầu lông gồm 7 nội dung thi đấu: 3 nội dung nam, 3 nội dung nữ và 1 nội dung đôi nam, nữ Nam: Đơn, Đôi, Đồng đội Nữ: Đơn, Đôi, Đồng đội Phối hợp: Đôi nam nữ Danh sách đội tuyển Cầu lông Việt Nam tại SEA Games 2022 Mỗi đoàn thể thao (NOC) được đăng ký tối đa 10 vận động viên nam và 10 vận động viên nữ ở môn Cầu lông. Nội dung đơn: Mỗi NOC được cử tối đa 2) vận động viên nam và 2 vận động viên nữ. Nội dung đôi: Mỗi NOC được cử tối đa 2 đôi cho mỗi nội dung đôi trong 3 nội dung thi đấu đôi. T.G', 'source': 'https://thethaovanhoa.vn/news-20220516095325404.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31.'}\n",
            "{'title': 'Lịch thi đấu tennis SEA Games 31 mới nhất', 'author': None, 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-18', 'categories': 'Thể thao', 'tags': 'Lịch thi đấu tennis SEA Games 31, VTV6 trực tiếp quần vợt, SEA Games 31, tennis, quần vợt, tennis SEA Games 31, quần vợt SEA Games 31, lich thi dau tennis, VTV6;Lịch thi đấu tennis SEA Games 31;VTV6 trực tiếp quần vợt;SEA Games 31;tennis;quần vợt;tennis SEA Games 31;quần vợt SEA Games 31;lich thi dau tennis;VTV6', 'fingerprint': 'ICHEeqL4EwYTsXOW5i/f2oj1vj4=', 'id': '4252224d', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay. Kết quả bóng đá SEA Games 31. Kết quả bóng đá U23 Việt Nam mới nhất. Kết quả bóng đá SEA Games 2022. Kết quả bóng đá SEA Games hôm nay. Lịch thi đấu tennis SEA Games 31 ngày 17/5: * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) Lịch thi đấu tennis SEA Games 31: * 10h ngày 13/5: Vòng 1 nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 14/5: Bán kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 15/5: Chung kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 18/5: Tứ kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 19/5: Vòng 1 nội dung đơn nam/đơn nữ; Bán kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 20/5: Tứ kết nội dung đơn nam/đơn nữ; Chung kết đôi nam nữ (VTV6, VTV5) * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) * 10h ngày 22/5: Chung kết nội dung đơn nam/đơn nữ (VTV6, VTV5) Tại SEA Games 31, nội dung tennis sẽ diễn ra từ 13-22/5/2022 ở cụm sân quần vợt Hanaka Paris Ocean Park thuộc thành phố Từ Sơn, tỉnh Bắc Ninh. Dự kiến các nội dung đồng đội sẽ đấu từ ngày 13-15/5 trước lúc tranh huy chương vào ngày 16/5. Từ 17-20/5 là thời gian dành cho các vòng đấu đôi và đơn, trước lúc tranh huy chương vào các ngày 21-22/5. Tay vợt nam số 1 Việt Nam Lý Hoàng Nam sẽ bảo vệ danh hiệu vô địch giành được ở SEA Games 30 tại Philippines. V.M', 'source': 'https://thethaovanhoa.vn/news-20220516210251757.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay.'}\n",
            "{'title': 'Lịch thi đấu và trực tiếp tennis SEA Games 31 hôm nay', 'author': None, 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-21', 'categories': 'Thể thao', 'tags': 'Lịch thi đấu tennis SEA Games 31, VTV6 trực tiêp quần vợt, SEA Games 31, tennis, quần vợt, tennis SEA Games 31, quần vợt SEA Games 31, lich thi dau tennis, VTV6;Lịch thi đấu tennis SEA Games 31;VTV6 trực tiêp quần vợt;SEA Games 31;tennis;quần vợt;tennis SEA Games 31;quần vợt SEA Games 31;lich thi dau tennis;VTV6', 'fingerprint': 'oqZmwF8xACPFIbMP9ngqvUjfswE=', 'id': '4e70bc85', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay. TRỰC TIẾP SEA Games 31 hôm nay ngày 21/5. Cập nhật bảng xếp hạng, bảng tổng sắp huy chương và lịch thi đấu của đoàn thể thao Việt Nam. Lịch thi đấu tennis SEA Games 31 ngày 21/5: * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) Lịch thi đấu tennis SEA Games 31: * 10h ngày 13/5: Vòng 1 nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 14/5: Bán kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 15/5: Chung kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 18/5: Tứ kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 19/5: Vòng 1 nội dung đơn nam/đơn nữ; Bán kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 20/5: Tứ kết nội dung đơn nam/đơn nữ; Chung kết đôi nam nữ (VTV6, VTV5) * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) * 10h ngày 22/5: Chung kết nội dung đơn nam/đơn nữ (VTV6, VTV5) Tại SEA Games 31, nội dung tennis sẽ diễn ra từ 13-22/5/2022 ở cụm sân quần vợt Hanaka Paris Ocean Park thuộc thành phố Từ Sơn, tỉnh Bắc Ninh. Dự kiến các nội dung đồng đội sẽ đấu từ ngày 13-15/5 trước lúc tranh huy chương vào ngày 16/5. Từ 17-20/5 là thời gian dành cho các vòng đấu đôi và đơn, trước lúc tranh huy chương vào các ngày 21-22/5. Tay vợt nam số 1 Việt Nam Lý Hoàng Nam sẽ bảo vệ danh hiệu vô địch giành được ở SEA Games 30 tại Philippines. V.M', 'source': 'https://thethaovanhoa.vn/news-20220517211439608.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay.'}\n",
            "{'title': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5)', 'author': 'PV; BTV-TTVH', 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-19', 'categories': 'Thể thao', 'tags': 'Lich thi dau Sea games 31, lich thi dau sea games, trực tiếp cầu lông Sea games 31, xem trực tiếp cầu lông sea games, lich thi dau cau long, trực tiếp cầu lông việt nam;Lich thi dau Sea games 31;lich thi dau sea games;trực tiếp cầu lông Sea games 31;xem trực tiếp cầu lông sea games;lich thi dau cau long;trực tiếp cầu lông việt nam', 'fingerprint': 'j+u6ya16H0vdxcOe4epzsKjBhXE=', 'id': '91ec24fb', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5) Thứ Năm, 19/05/2022 10:55 GMT+7 Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31. Lịch thi đấu cầu lông SEA Games 31 - Xem VTV6 VTV5 trực tiếp cầu lông SEA Games 31 năm 2022. Lịch thi đấu môn cầu lông Việt Nam SEA Games 31. Xem trực tiếp cầu lông SEA Games Lịch thi đấu cầu lông SEA Games 31: Ngày 18/5: 13h00 - 16h00: Đồng đội nữ 18h00 - 21h00: Đồng đội nam Ngày 19/5: 9h00 - 21h00: Vòng 1/16 đơn nam Vòng 1/16 đơn nữ Vòng 1/16 đôi nam Vòng 1/16 đôi nữ Vòng 1/16 đôi nam nữ Ngày 20/5: 9h00 - 17h00: Tứ kết đơn nam Tứ kết đơn nữ Tứ kết đôi nam Tứ kết đôi nữ Tứ kết đôi nam nữ Ngày 21/5: 13h00 - 18h00 Bán kết đơn nam Bán kết đơn nữ Bán kết đôi nam Bán kết đôi nữ Bán kết đôi nam nữ Ngày 22/5: Từ 12h00: Chung kết đơn nam Trao huy chương đơn nam Chung kết đơn nữ Trao huy chương đơn nữ Chung kết đôi nam Trao huy chương đôi nam Chung kết đôi nữ Trao huy chương đôi nữ Chung kết đôi nam nữ Trao huy chương đôi nam nữ Cầu lông SEA Games 31 diễn ra khi nào, ở đâu? Cầu lông SEA Games 31 diễn ra từ ngày 16/5 đến ngày 22/5 tại Nhà thi đấu thể thao Bắc Giang. Các nội dung Cầu lông SEA Games 2022 Môn Cầu lông gồm 7 nội dung thi đấu: 3 nội dung nam, 3 nội dung nữ và 1 nội dung đôi nam, nữ Nam: Đơn, Đôi, Đồng đội Nữ: Đơn, Đôi, Đồng đội Phối hợp: Đôi nam nữ Danh sách đội tuyển Cầu lông Việt Nam tại SEA Games 2022 Mỗi đoàn thể thao (NOC) được đăng ký tối đa 10 vận động viên nam và 10 vận động viên nữ ở môn Cầu lông. Nội dung đơn: Mỗi NOC được cử tối đa 2) vận động viên nam và 2 vận động viên nữ. Nội dung đôi: Mỗi NOC được cử tối đa 2 đôi cho mỗi nội dung đôi trong 3 nội dung thi đấu đôi. T.G', 'source': 'https://thethaovanhoa.vn/news-20220516095325404.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31.'}\n",
            "{'title': 'Lịch thi đấu tennis SEA Games 31 mới nhất', 'author': None, 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-18', 'categories': 'Thể thao', 'tags': 'Lịch thi đấu tennis SEA Games 31, VTV6 trực tiếp quần vợt, SEA Games 31, tennis, quần vợt, tennis SEA Games 31, quần vợt SEA Games 31, lich thi dau tennis, VTV6;Lịch thi đấu tennis SEA Games 31;VTV6 trực tiếp quần vợt;SEA Games 31;tennis;quần vợt;tennis SEA Games 31;quần vợt SEA Games 31;lich thi dau tennis;VTV6', 'fingerprint': 'ICHEeqL4EwYTsXOW5i/f2oj1vj4=', 'id': '4252224d', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay. Kết quả bóng đá SEA Games 31. Kết quả bóng đá U23 Việt Nam mới nhất. Kết quả bóng đá SEA Games 2022. Kết quả bóng đá SEA Games hôm nay. Lịch thi đấu tennis SEA Games 31 ngày 17/5: * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) Lịch thi đấu tennis SEA Games 31: * 10h ngày 13/5: Vòng 1 nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 14/5: Bán kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 15/5: Chung kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 18/5: Tứ kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 19/5: Vòng 1 nội dung đơn nam/đơn nữ; Bán kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 20/5: Tứ kết nội dung đơn nam/đơn nữ; Chung kết đôi nam nữ (VTV6, VTV5) * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) * 10h ngày 22/5: Chung kết nội dung đơn nam/đơn nữ (VTV6, VTV5) Tại SEA Games 31, nội dung tennis sẽ diễn ra từ 13-22/5/2022 ở cụm sân quần vợt Hanaka Paris Ocean Park thuộc thành phố Từ Sơn, tỉnh Bắc Ninh. Dự kiến các nội dung đồng đội sẽ đấu từ ngày 13-15/5 trước lúc tranh huy chương vào ngày 16/5. Từ 17-20/5 là thời gian dành cho các vòng đấu đôi và đơn, trước lúc tranh huy chương vào các ngày 21-22/5. Tay vợt nam số 1 Việt Nam Lý Hoàng Nam sẽ bảo vệ danh hiệu vô địch giành được ở SEA Games 30 tại Philippines. V.M', 'source': 'https://thethaovanhoa.vn/news-20220516210251757.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay.'}\n",
            "{'title': 'Lịch thi đấu và trực tiếp tennis SEA Games 31 hôm nay', 'author': None, 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-21', 'categories': 'Thể thao', 'tags': 'Lịch thi đấu tennis SEA Games 31, VTV6 trực tiêp quần vợt, SEA Games 31, tennis, quần vợt, tennis SEA Games 31, quần vợt SEA Games 31, lich thi dau tennis, VTV6;Lịch thi đấu tennis SEA Games 31;VTV6 trực tiêp quần vợt;SEA Games 31;tennis;quần vợt;tennis SEA Games 31;quần vợt SEA Games 31;lich thi dau tennis;VTV6', 'fingerprint': 'oqZmwF8xACPFIbMP9ngqvUjfswE=', 'id': '4e70bc85', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay. TRỰC TIẾP SEA Games 31 hôm nay ngày 21/5. Cập nhật bảng xếp hạng, bảng tổng sắp huy chương và lịch thi đấu của đoàn thể thao Việt Nam. Lịch thi đấu tennis SEA Games 31 ngày 21/5: * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) Lịch thi đấu tennis SEA Games 31: * 10h ngày 13/5: Vòng 1 nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 14/5: Bán kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 15/5: Chung kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 18/5: Tứ kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 19/5: Vòng 1 nội dung đơn nam/đơn nữ; Bán kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 20/5: Tứ kết nội dung đơn nam/đơn nữ; Chung kết đôi nam nữ (VTV6, VTV5) * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) * 10h ngày 22/5: Chung kết nội dung đơn nam/đơn nữ (VTV6, VTV5) Tại SEA Games 31, nội dung tennis sẽ diễn ra từ 13-22/5/2022 ở cụm sân quần vợt Hanaka Paris Ocean Park thuộc thành phố Từ Sơn, tỉnh Bắc Ninh. Dự kiến các nội dung đồng đội sẽ đấu từ ngày 13-15/5 trước lúc tranh huy chương vào ngày 16/5. Từ 17-20/5 là thời gian dành cho các vòng đấu đôi và đơn, trước lúc tranh huy chương vào các ngày 21-22/5. Tay vợt nam số 1 Việt Nam Lý Hoàng Nam sẽ bảo vệ danh hiệu vô địch giành được ở SEA Games 30 tại Philippines. V.M', 'source': 'https://thethaovanhoa.vn/news-20220517211439608.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay.'}\n",
            "{'title': 'Xem trực tiếp bóng rổ SEA Games 31', 'author': 'PV; BTV-TTVH', 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-14', 'categories': 'Thể thao', 'tags': 'truc tiep bong ro, VTV6, VTV5 xem trực tiếp bóng rổ SEA Games 31, bong ro Viet Nam, trực tiếp bóng rổ hôm nay, xem trực tiếp bóng rổ Seagames 31;truc tiep bong ro;VTV6;VTV5 xem trực tiếp bóng rổ SEA Games 31;bong ro Viet Nam;trực tiếp bóng rổ hôm nay;xem trực tiếp bóng rổ Seagames 31', 'fingerprint': 'hfx4NGhvD/DOF5ucQvf8mAbC8TE=', 'id': 'f7ac473b', 'license': None, 'comments': None, 'raw_text': 'Xem trực tiếp bóng rổ SEA Games 31. VTV6, VTV5 trực tiếp bóng rổ SEA Games. Lịch trực tiếp bóng rổ SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn bóng rổ SEA Games 31. TRỰC TIẾP SEA GAMES 31 NGÀY 14/5: Đội tuyển nữ Việt Nam sẽ lấy vé vào bán kết, điền kinh, bơi lội nhập cuộc, đua thuyền, thể dục dụng cụ, wushu, eSports tranh huy chương. Xem trực tiếp bóng rổ SEA Games 31 Môn bóng rổ sẽ diễn ra từ ngày 13/05 đến hết ngày 22/05, cụ thể như sau: - Bóng rổ 3x3 sẽ tranh tài vào hai ngày 13/05 và 14/05, trao huy chương vào ngày 14. - Sau một ngày nghỉ để chuyển đổi sân thi đấu 3x3 thành 5x5, nội dung bóng rổ 5x5 sẽ diễn ra từ ngày 15/05 đến ngày 22/05, trao huy chương vào ngày 22. Lịch thi đấu của riêng đội tuyển bóng rổ 3x3 Việt Nam: Nội dung nam: Ngày 13/5 - 09h20 - Việt Nam vs Campuchia Ngày 13/5 - 11h40 - Việt Nam vs Thái Lan Ngày 13/5 - 14h20 - Việt Nam vs Indonesia Ngày 13/5 - 16h00 - Việt Nam vs Singapore Ngày 14/5 - 10h00 - Việt Nam vs Malaysia Ngày 14/5 - 14h00 - Việt Nam vs Philippines Nội dung nữ: Ngày 13/5 - 10h40 -Việt Nam vs Indonesia Ngày 13/5 - 13h20 -Việt Nam vs Malaysia Ngày 13/5 - 15h00 -Việt Nam vs Philippines Ngày 13/5 - 16h40 - Việt Nam vs Campuchia Ngày 14/5 - 14h00 - Việt Nam vs Thái Lan Ngày 14/5 - 14h00 - Việt Nam vs Singapore Thể thức thi đấu: Với 7 đội tuyển nam 3x3 và 7 đội tuyển nữ 3x3 tranh tài ở SEA Games 31, thể thức thi đấu được chọn là vòng tròn 1 lượt tính điểm ở cả hai nội dung. Sau đó, bốn đội có thứ hạng cao nhất sẽ thi đấu loạt trận bán kết theo cặp đấu 1-4 và 2-3 (đội hạng 1 gặp đội hạng 4, đội hạng 2 gặp đội hạng 3). Hai đội thắng ở bán kết sẽ đối đầu ở trận chung kết để tìm ra chủ nhân của tấm huy chương vàng. Trong khi đó, hai đội thua bán kết sẽ thi đấu để tranh huy chương đồng. V.M', 'source': 'https://thethaovanhoa.vn/news-20220514092947034.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Xem trực tiếp bóng rổ SEA Games 31. VTV6, VTV5 trực tiếp bóng rổ SEA Games. Lịch trực tiếp bóng rổ SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn bóng rổ SEA Games 31.'}\n",
            "{'title': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5)', 'author': 'PV; BTV-TTVH', 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-19', 'categories': 'Thể thao', 'tags': 'Lich thi dau Sea games 31, lich thi dau sea games, trực tiếp cầu lông Sea games 31, xem trực tiếp cầu lông sea games, lich thi dau cau long, trực tiếp cầu lông việt nam;Lich thi dau Sea games 31;lich thi dau sea games;trực tiếp cầu lông Sea games 31;xem trực tiếp cầu lông sea games;lich thi dau cau long;trực tiếp cầu lông việt nam', 'fingerprint': 'j+u6ya16H0vdxcOe4epzsKjBhXE=', 'id': '91ec24fb', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 (VTV6, VTV5) Thứ Năm, 19/05/2022 10:55 GMT+7 Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31. Lịch thi đấu cầu lông SEA Games 31 - Xem VTV6 VTV5 trực tiếp cầu lông SEA Games 31 năm 2022. Lịch thi đấu môn cầu lông Việt Nam SEA Games 31. Xem trực tiếp cầu lông SEA Games Lịch thi đấu cầu lông SEA Games 31: Ngày 18/5: 13h00 - 16h00: Đồng đội nữ 18h00 - 21h00: Đồng đội nam Ngày 19/5: 9h00 - 21h00: Vòng 1/16 đơn nam Vòng 1/16 đơn nữ Vòng 1/16 đôi nam Vòng 1/16 đôi nữ Vòng 1/16 đôi nam nữ Ngày 20/5: 9h00 - 17h00: Tứ kết đơn nam Tứ kết đơn nữ Tứ kết đôi nam Tứ kết đôi nữ Tứ kết đôi nam nữ Ngày 21/5: 13h00 - 18h00 Bán kết đơn nam Bán kết đơn nữ Bán kết đôi nam Bán kết đôi nữ Bán kết đôi nam nữ Ngày 22/5: Từ 12h00: Chung kết đơn nam Trao huy chương đơn nam Chung kết đơn nữ Trao huy chương đơn nữ Chung kết đôi nam Trao huy chương đôi nam Chung kết đôi nữ Trao huy chương đôi nữ Chung kết đôi nam nữ Trao huy chương đôi nam nữ Cầu lông SEA Games 31 diễn ra khi nào, ở đâu? Cầu lông SEA Games 31 diễn ra từ ngày 16/5 đến ngày 22/5 tại Nhà thi đấu thể thao Bắc Giang. Các nội dung Cầu lông SEA Games 2022 Môn Cầu lông gồm 7 nội dung thi đấu: 3 nội dung nam, 3 nội dung nữ và 1 nội dung đôi nam, nữ Nam: Đơn, Đôi, Đồng đội Nữ: Đơn, Đôi, Đồng đội Phối hợp: Đôi nam nữ Danh sách đội tuyển Cầu lông Việt Nam tại SEA Games 2022 Mỗi đoàn thể thao (NOC) được đăng ký tối đa 10 vận động viên nam và 10 vận động viên nữ ở môn Cầu lông. Nội dung đơn: Mỗi NOC được cử tối đa 2) vận động viên nam và 2 vận động viên nữ. Nội dung đôi: Mỗi NOC được cử tối đa 2 đôi cho mỗi nội dung đôi trong 3 nội dung thi đấu đôi. T.G', 'source': 'https://thethaovanhoa.vn/news-20220516095325404.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu và trực tiếp cầu lông SEA Games 31 - Xem trực tiếp cầu lông SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn cầu lông SEA Games 31.'}\n",
            "{'title': 'Lịch thi đấu tennis SEA Games 31 mới nhất', 'author': None, 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-18', 'categories': 'Thể thao', 'tags': 'Lịch thi đấu tennis SEA Games 31, VTV6 trực tiếp quần vợt, SEA Games 31, tennis, quần vợt, tennis SEA Games 31, quần vợt SEA Games 31, lich thi dau tennis, VTV6;Lịch thi đấu tennis SEA Games 31;VTV6 trực tiếp quần vợt;SEA Games 31;tennis;quần vợt;tennis SEA Games 31;quần vợt SEA Games 31;lich thi dau tennis;VTV6', 'fingerprint': 'ICHEeqL4EwYTsXOW5i/f2oj1vj4=', 'id': '4252224d', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay. Kết quả bóng đá SEA Games 31. Kết quả bóng đá U23 Việt Nam mới nhất. Kết quả bóng đá SEA Games 2022. Kết quả bóng đá SEA Games hôm nay. Lịch thi đấu tennis SEA Games 31 ngày 17/5: * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) Lịch thi đấu tennis SEA Games 31: * 10h ngày 13/5: Vòng 1 nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 14/5: Bán kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 15/5: Chung kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 18/5: Tứ kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 19/5: Vòng 1 nội dung đơn nam/đơn nữ; Bán kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 20/5: Tứ kết nội dung đơn nam/đơn nữ; Chung kết đôi nam nữ (VTV6, VTV5) * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) * 10h ngày 22/5: Chung kết nội dung đơn nam/đơn nữ (VTV6, VTV5) Tại SEA Games 31, nội dung tennis sẽ diễn ra từ 13-22/5/2022 ở cụm sân quần vợt Hanaka Paris Ocean Park thuộc thành phố Từ Sơn, tỉnh Bắc Ninh. Dự kiến các nội dung đồng đội sẽ đấu từ ngày 13-15/5 trước lúc tranh huy chương vào ngày 16/5. Từ 17-20/5 là thời gian dành cho các vòng đấu đôi và đơn, trước lúc tranh huy chương vào các ngày 21-22/5. Tay vợt nam số 1 Việt Nam Lý Hoàng Nam sẽ bảo vệ danh hiệu vô địch giành được ở SEA Games 30 tại Philippines. V.M', 'source': 'https://thethaovanhoa.vn/news-20220516210251757.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay.'}\n",
            "{'title': 'Lịch thi đấu và trực tiếp tennis SEA Games 31 hôm nay', 'author': None, 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-21', 'categories': 'Thể thao', 'tags': 'Lịch thi đấu tennis SEA Games 31, VTV6 trực tiêp quần vợt, SEA Games 31, tennis, quần vợt, tennis SEA Games 31, quần vợt SEA Games 31, lich thi dau tennis, VTV6;Lịch thi đấu tennis SEA Games 31;VTV6 trực tiêp quần vợt;SEA Games 31;tennis;quần vợt;tennis SEA Games 31;quần vợt SEA Games 31;lich thi dau tennis;VTV6', 'fingerprint': 'oqZmwF8xACPFIbMP9ngqvUjfswE=', 'id': '4e70bc85', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay. TRỰC TIẾP SEA Games 31 hôm nay ngày 21/5. Cập nhật bảng xếp hạng, bảng tổng sắp huy chương và lịch thi đấu của đoàn thể thao Việt Nam. Lịch thi đấu tennis SEA Games 31 ngày 21/5: * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) Lịch thi đấu tennis SEA Games 31: * 10h ngày 13/5: Vòng 1 nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 14/5: Bán kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 15/5: Chung kết nội dung đồng đội nam/đồng đội nữ (VTV6, VTV5) * 10h ngày 17/5: Vòng 1 nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 18/5: Tứ kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 19/5: Vòng 1 nội dung đơn nam/đơn nữ; Bán kết nội dung đôi nam/đôi nữ/đôi nam nữ (VTV6, VTV5) * 10h ngày 20/5: Tứ kết nội dung đơn nam/đơn nữ; Chung kết đôi nam nữ (VTV6, VTV5) * 10h ngày 21/5: Bán kết nội dung đơn nam/đơn nữ; Chung kết đôi nam/đôi nữ (VTV6, VTV5) * 10h ngày 22/5: Chung kết nội dung đơn nam/đơn nữ (VTV6, VTV5) Tại SEA Games 31, nội dung tennis sẽ diễn ra từ 13-22/5/2022 ở cụm sân quần vợt Hanaka Paris Ocean Park thuộc thành phố Từ Sơn, tỉnh Bắc Ninh. Dự kiến các nội dung đồng đội sẽ đấu từ ngày 13-15/5 trước lúc tranh huy chương vào ngày 16/5. Từ 17-20/5 là thời gian dành cho các vòng đấu đôi và đơn, trước lúc tranh huy chương vào các ngày 21-22/5. Tay vợt nam số 1 Việt Nam Lý Hoàng Nam sẽ bảo vệ danh hiệu vô địch giành được ở SEA Games 30 tại Philippines. V.M', 'source': 'https://thethaovanhoa.vn/news-20220517211439608.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu tennis SEA Games 31 mới nhất. Lịch thi đấu quần vợt SEA Games 31. Lịch trực tiếp tennis SEA Games 31 hôm nay.'}\n",
            "{'title': 'Xem trực tiếp bóng rổ SEA Games 31', 'author': 'PV; BTV-TTVH', 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-14', 'categories': 'Thể thao', 'tags': 'truc tiep bong ro, VTV6, VTV5 xem trực tiếp bóng rổ SEA Games 31, bong ro Viet Nam, trực tiếp bóng rổ hôm nay, xem trực tiếp bóng rổ Seagames 31;truc tiep bong ro;VTV6;VTV5 xem trực tiếp bóng rổ SEA Games 31;bong ro Viet Nam;trực tiếp bóng rổ hôm nay;xem trực tiếp bóng rổ Seagames 31', 'fingerprint': 'hfx4NGhvD/DOF5ucQvf8mAbC8TE=', 'id': 'f7ac473b', 'license': None, 'comments': None, 'raw_text': 'Xem trực tiếp bóng rổ SEA Games 31. VTV6, VTV5 trực tiếp bóng rổ SEA Games. Lịch trực tiếp bóng rổ SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn bóng rổ SEA Games 31. TRỰC TIẾP SEA GAMES 31 NGÀY 14/5: Đội tuyển nữ Việt Nam sẽ lấy vé vào bán kết, điền kinh, bơi lội nhập cuộc, đua thuyền, thể dục dụng cụ, wushu, eSports tranh huy chương. Xem trực tiếp bóng rổ SEA Games 31 Môn bóng rổ sẽ diễn ra từ ngày 13/05 đến hết ngày 22/05, cụ thể như sau: - Bóng rổ 3x3 sẽ tranh tài vào hai ngày 13/05 và 14/05, trao huy chương vào ngày 14. - Sau một ngày nghỉ để chuyển đổi sân thi đấu 3x3 thành 5x5, nội dung bóng rổ 5x5 sẽ diễn ra từ ngày 15/05 đến ngày 22/05, trao huy chương vào ngày 22. Lịch thi đấu của riêng đội tuyển bóng rổ 3x3 Việt Nam: Nội dung nam: Ngày 13/5 - 09h20 - Việt Nam vs Campuchia Ngày 13/5 - 11h40 - Việt Nam vs Thái Lan Ngày 13/5 - 14h20 - Việt Nam vs Indonesia Ngày 13/5 - 16h00 - Việt Nam vs Singapore Ngày 14/5 - 10h00 - Việt Nam vs Malaysia Ngày 14/5 - 14h00 - Việt Nam vs Philippines Nội dung nữ: Ngày 13/5 - 10h40 -Việt Nam vs Indonesia Ngày 13/5 - 13h20 -Việt Nam vs Malaysia Ngày 13/5 - 15h00 -Việt Nam vs Philippines Ngày 13/5 - 16h40 - Việt Nam vs Campuchia Ngày 14/5 - 14h00 - Việt Nam vs Thái Lan Ngày 14/5 - 14h00 - Việt Nam vs Singapore Thể thức thi đấu: Với 7 đội tuyển nam 3x3 và 7 đội tuyển nữ 3x3 tranh tài ở SEA Games 31, thể thức thi đấu được chọn là vòng tròn 1 lượt tính điểm ở cả hai nội dung. Sau đó, bốn đội có thứ hạng cao nhất sẽ thi đấu loạt trận bán kết theo cặp đấu 1-4 và 2-3 (đội hạng 1 gặp đội hạng 4, đội hạng 2 gặp đội hạng 3). Hai đội thắng ở bán kết sẽ đối đầu ở trận chung kết để tìm ra chủ nhân của tấm huy chương vàng. Trong khi đó, hai đội thua bán kết sẽ thi đấu để tranh huy chương đồng. V.M', 'source': 'https://thethaovanhoa.vn/news-20220514092947034.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Xem trực tiếp bóng rổ SEA Games 31. VTV6, VTV5 trực tiếp bóng rổ SEA Games. Lịch trực tiếp bóng rổ SEA Games 31 năm 2022 tại Việt Nam. Lịch thi đấu môn bóng rổ SEA Games 31.'}\n",
            "{'title': 'Lịch thi đấu SEA Games 31 hôm nay 21/5. Chung kết bóng đá nữ Việt Nam vs Thái Lan', 'author': 'PV; BTV-TTVH', 'hostname': 'thethaovanhoa.vn', 'date': '2022-05-21', 'categories': 'Lịch thi đấu', 'tags': 'lịch thi đấu SEA Games 31, lịch thi đấu seagame, vtv6, vtv5, xem vtv6, xem vtv5, trực tiếp SEA Games, trực tiếp seagame, bóng đá nam, futsal nam, bóng chuyền SEA Games;lịch thi đấu SEA Games 31;lịch thi đấu seagame;vtv6;vtv5;xem vtv6;xem vtv5;trực tiếp SEA Games;trực tiếp seagame;bóng đá nam;futsal nam;bóng chuyền SEA Games', 'fingerprint': 'rXT2G8PG92AqxrAlGtLMUpb2UXc=', 'id': '9c35c145', 'license': None, 'comments': None, 'raw_text': 'Lịch thi đấu SEA Games 31 hôm nay 21/5: Bóng đá nữ Việt Nam gặp Thái Lan. Bóng rổ 5x5. Cầu lông. Cờ vua, xe đạp, bóng ném, Judo, Muay, quần vợt, bắn súng. VTV6 TRỰC TIẾP bóng đá nữ Việt Nam vs Thái Lan, chung kết SEA Games 31 (19h00, 21/5). Xem bóng đá trực tiếp nữ VN vs Thái. VTV6 trực tiếp bóng đá Seagame 31 hôm nay. ........................... LỊCH THI ĐẤU NGÀY 21/5 1. Bóng đá nữ (tại sân Cẩm Phả, Quảng Ninh): Tranh hạng Ba: Myanmar - Philippines (16h00) Chung kết: Việt Nam - Thái Lan (19h00) 2. Cầu lông (tại NTĐ tỉnh Bắc Giang): Bán kết cá nhân nam; cá nhân nữ; đôi nam; đôi nữ; đôi nam/nữ (từ 13h00 đến 18h00) 3. Bóng rổ 5x5 (tại NTĐ Thanh Trì, Hà Nội): Đội tuyển nam và đội tuyển nữ (từ 9h00 đến 21h00) 4. Billiards - Snooker (tại NTĐ Hà Đông, Hà Nội): Vòng loại Snooker đơn nam; Carom 3 băng nam; Pool 10 bi năm (từ 10h00 đến 19h00) 5. Boxing (tại NTĐ tỉnh Bắc Ninh): Bán kết các hạng cân nam, nữ (từ 14h00 đến 18h00) 6. Canoeing/Kayak (tại Khu đua thuyền Sông Giá, Hải Phòng): Vòng loại đên chung kết các nội dung 200m (từ 9h30 đến 12h00) Chung kết các nội dung 500m còn lại (từ 12h45 đến 13h15) 7. Cờ Vua (tại Cung quy hoạch, hội chợ và triển lãm tỉnh Quảng Ninh) Vòng 1 đến 7 (vòng cuối) cờ chớp nhoáng đồng đội nam, nữ (Từ 10h00 đến 13h00) 8. Xe đạp (tại Hòa Bình): Thi đấu Xuất phát đồng hành nam cá nhân (từ 9h00 đến 14h00) 9. Aerobic (tại Cung thể thao Quần Ngựa Hà Nội): Chung kết đơn nam (từ 10h00 đến 10h30) Chung kết nội dung 3 người (từ 11h30 đến 12h00) 10. Bóng ném (tại Đại học TDTT Từ Sơn, Bắc Ninh): Đấu vòng tròn nam, nữ (từ 14h00 đến 18h00) 11. Judo (tại NTĐ huyện Hoài Đức, Hà Nội): Vòng loại đến chung kết các hạng cân: -55kg nam; -45kg, -48kg nữ (từ 13h00 đến 17h30) 12. Muay (tại Trung tâm TDTT tỉnh Vĩnh Phúc): Bán kết các hạng cân đối kháng (từ 14h00 đến 18h00) 13. Cầu mây (tại NTĐ Hoàng Mai Hà Nội) Chung kết các nội dung: Đồng đội 4 người nam; Đồng đội 4 người nữ (từ 13h00 đến 17h00) 14. Bắn súng (tại Trung tâm HLTTQG Nhổn, Hà Nội) Thi đấu các nội dung: 50m súng trường 3 tư thế nam; 10m súng ngắn hơi đồng đội hỗn hợp; 10m súng trường hơi mục tiêu di động nữ; Trap Nam - Ngày 1 (Từ 9h00 đến 16h00) 15. Quần vợt (tại Hanaka Từ Sơn, Bắc Ninh) Bán kết Đơn nam; Đơn nữ (từ 10h00) Chung kết Đôi nam; Đôi nữ (từ 10h00) 16. Bóng chuyền (tại NTĐ tỉnh Quảng Ninh) Bán kết nam, nữ (từ 10h00 đến 22h00) 17. Cử tạ (tại Trung tâm huấn luyện và thi đấu TDTT Hà Nội) Thi đấu các hạng cân: 64kg nữ; 73kg, 81kg nam (từ 10h00 đến 14h00) 18. Thể thao điện tử (tại Trung tâm Hội nghị quốc gia Hà Nội) Vòng bảng Mobile Legends: Bang Bang (từ 10h30 đến 18h30) Vòng bảng Đột kích (từ 10h30 đến 18h30) 19. Lặn (tại Cung thể thao dưới nước Mỹ Đình, Hà Nội) Vòng loại đến chung kết các nội dung: 100m nam; 100m nữ; 100m bi-fins nam; 100m bi-fins nữ; 1.500m nam; Tiếp sức 4x200m nữ; Tiếp sức 4x200m nam (từ 9h00 đến 18h00). 2-Vovinam (tại NTĐ Sóc Sơn, Hà Nội) Thi đấu các nội dung đối kháng và biểu diễn quyền (từ 14h00) Lâm Chi', 'source': 'https://thethaovanhoa.vn/news-20220521053123934.htm', 'source-hostname': 'thethaovanhoa.vn', 'excerpt': 'Lịch thi đấu SEA Games 31 hôm nay 21/5: Bóng đá nữ Việt Nam gặp Thái Lan. Bóng rổ 5x5. Cầu lông. Cờ vua, xe đạp, bóng ném, Judo, Muay, quần vợt, bắn súng.'}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "query=\"seagames\"\n",
        "max_results=5\n",
        "lookup_period=\"3Y\"\n",
        "language=\"vi\"\n",
        "country=\"VN\"\n",
        "userName = []\n",
        "content = []\n",
        "rating = []\n",
        "id = []\n",
        "title = []\n",
        "platform = []\n",
        "extracted_data=[]\n",
        "source_config = GoogleNewsConfig(\n",
        "query=query,\n",
        "max_results=max_results,\n",
        "lookup_period=lookup_period,\n",
        "language=language,\n",
        "country=country,\n",
        ")\n",
        "source = GoogleNewsSource()\n",
        "data = source.lookup(source_config)\n",
        "#print(data)\n",
        "for i in range(0, max_results):\n",
        "    data[i]=dict(data[i])\n",
        "    #print(data[i])\n",
        "    id.append(None)\n",
        "    rating.append(None)\n",
        "    platform.append('Google News')\n",
        "    extracted_data.append(data[i][\"meta\"][\"extracted_data\"])\n",
        "    Not_none_values = filter(None.__ne__, extracted_data)\n",
        "    extracted_data = list(Not_none_values)\n",
        "    for i in range(len(extracted_data)):\n",
        "      extracted_data[i]=dict(extracted_data[i])\n",
        "      print(extracted_data[i])\n",
        "      userName.append(extracted_data[i][\"author\"])\n",
        "      title.append(extracted_data[i][\"title\"])\n",
        "      content.append(extracted_data[i][\"raw_text\"])\n",
        "data=pd.DataFrame(list(zip(title,userName,content)),columns=[\"title\",\"author\",\"content\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEty8uU2ucoj"
      },
      "outputs": [],
      "source": [
        "data=data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "v2O2KKrwIOpU",
        "outputId": "2a327fe5-02dc-4159-c19c-c5fb83696656"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-370c35bd-255d-49a5-a73f-6cdcabd4f535\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Đảm bảo mức cao nhất trong công tác y tế phục ...</td>\n",
              "      <td>Lê Hảo TTXVN; Vietnam</td>\n",
              "      <td>Đảm bảo mức cao nhất trong công tác y tế phục ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IPC không cho phép Nga, Belarus tham gia Paral...</td>\n",
              "      <td>Thúc Anh TTXVN; Vietnam</td>\n",
              "      <td>IPC không cho phép Nga, Belarus tham gia Paral...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>U23 Đông Nam Á 2022: Tuyển Việt Nam bắt đầu hà...</td>\n",
              "      <td>Minh Tiến TTXVN; Vietnam</td>\n",
              "      <td>U23 Đông Nam Á 2022: Tuyển Việt Nam bắt đầu hà...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Phát hiện trên 27.000 trường hợp vi phạm nồng ...</td>\n",
              "      <td>Chu Thanh Vân TTXVN; Vietnam</td>\n",
              "      <td>Phát hiện trên 27.000 trường hợp vi phạm nồng ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-370c35bd-255d-49a5-a73f-6cdcabd4f535')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-370c35bd-255d-49a5-a73f-6cdcabd4f535 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-370c35bd-255d-49a5-a73f-6cdcabd4f535');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  Đảm bảo mức cao nhất trong công tác y tế phục ...   \n",
              "2  IPC không cho phép Nga, Belarus tham gia Paral...   \n",
              "5  U23 Đông Nam Á 2022: Tuyển Việt Nam bắt đầu hà...   \n",
              "9  Phát hiện trên 27.000 trường hợp vi phạm nồng ...   \n",
              "\n",
              "                         author  \\\n",
              "0         Lê Hảo TTXVN; Vietnam   \n",
              "2       Thúc Anh TTXVN; Vietnam   \n",
              "5      Minh Tiến TTXVN; Vietnam   \n",
              "9  Chu Thanh Vân TTXVN; Vietnam   \n",
              "\n",
              "                                             content  \n",
              "0  Đảm bảo mức cao nhất trong công tác y tế phục ...  \n",
              "2  IPC không cho phép Nga, Belarus tham gia Paral...  \n",
              "5  U23 Đông Nam Á 2022: Tuyển Việt Nam bắt đầu hà...  \n",
              "9  Phát hiện trên 27.000 trường hợp vi phạm nồng ...  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjGQ8VVPwmGE"
      },
      "source": [
        "# config website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PFN4j5gwW-K",
        "outputId": "08adaf49-27cc-4a0f-ed76-6c15e057cb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:obsei:\n",
            "By default `pip install obsei` will only install core dependencies.\n",
            "To install all required dependencies use `pip install obsei[all]`.\n",
            "Refer https://obsei.com/#install-obsei for more options.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import logging\n",
        "from abc import abstractmethod\n",
        "from typing import List, Optional\n",
        "\n",
        "import mmh3\n",
        "\n",
        "from obsei.payload import TextPayload\n",
        "from obsei.source.base_source import BaseSource, BaseSourceConfig\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class BaseCrawlerConfig(BaseSourceConfig):\n",
        "    TYPE: str = \"BaseCrawler\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def extract_url(self, url: str, url_id: str = None):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def find_urls(self, url: str):\n",
        "        pass\n",
        "\n",
        "\n",
        "class TrafilaturaCrawlerConfig(BaseCrawlerConfig):\n",
        "    # To understand about these configuration params refer:\n",
        "    # https://trafilatura.readthedocs.io/\n",
        "    _output_format: str = \"json\"\n",
        "    TYPE: str = \"Crawler\"\n",
        "    urls: List[str]\n",
        "    include_comments: bool = False\n",
        "    include_tables: bool = True\n",
        "    no_fallback: bool = False\n",
        "    include_images: bool = False\n",
        "    include_formatting: bool = False\n",
        "    deduplicate: bool = True\n",
        "    no_ssl: bool = False\n",
        "    is_feed: bool = False\n",
        "    is_sitemap: bool = False\n",
        "    include_links: bool = True\n",
        "    target_language: Optional[str] = None\n",
        "    url_blacklist: Optional[List[str]] = None\n",
        "\n",
        "    def extract_url(self, url: str, url_id: str = None):\n",
        "        try:\n",
        "            from trafilatura import extract, fetch_url\n",
        "        except:\n",
        "            logger.error(\"Trafilatura is not installed, install as follows: pip install trafilatura\")\n",
        "            return []\n",
        "\n",
        "        url_id = url_id or \"{:02x}\".format(mmh3.hash(url, signed=False))\n",
        "        url_content = fetch_url(\n",
        "            url=url,\n",
        "            no_ssl=self.no_ssl,\n",
        "        )\n",
        "        extracted_dict = None\n",
        "        if url_content is not None:\n",
        "            extracted_data = extract(\n",
        "                filecontent=url_content,\n",
        "                record_id=url_id,\n",
        "                no_fallback=self.no_fallback,\n",
        "                output_format=self._output_format,\n",
        "                include_comments=self.include_comments,\n",
        "                include_tables=self.include_tables,\n",
        "                include_images=self.include_images,\n",
        "                include_formatting=self.include_formatting,\n",
        "                include_links=self.include_links,\n",
        "                deduplicate=self.deduplicate,\n",
        "                url_blacklist=self.url_blacklist,\n",
        "                target_language=self.target_language,\n",
        "            )\n",
        "\n",
        "            if extracted_data:\n",
        "                extracted_dict = json.loads(extracted_data)\n",
        "                if \"raw-text\" in extracted_dict:\n",
        "                    del extracted_dict[\"raw-text\"]\n",
        "\n",
        "        return extracted_dict\n",
        "\n",
        "    def find_urls(self, url: str):\n",
        "        try:\n",
        "            from trafilatura import feeds, sitemaps\n",
        "        except:\n",
        "            logger.error(\"Trafilatura is not installed, install as follows: pip install trafilatura\")\n",
        "            return []\n",
        "\n",
        "        urls: List[str] = []\n",
        "        if self.is_sitemap:\n",
        "            urls = sitemaps.sitemap_search(url=url, target_lang=self.target_language)\n",
        "        elif self.is_feed:\n",
        "            urls = feeds.find_feed_urls(url=url, target_lang=self.target_language)\n",
        "\n",
        "        return urls\n",
        "\n",
        "\n",
        "class TrafilaturaCrawlerSource(BaseSource):\n",
        "    NAME: Optional[str] = \"Crawler\"\n",
        "\n",
        "    def lookup(  # type: ignore[override]\n",
        "        self, config: TrafilaturaCrawlerConfig, **kwargs\n",
        "    ) -> List[TextPayload]:\n",
        "        source_responses: List[TextPayload] = []\n",
        "\n",
        "        final_urls = []\n",
        "        if config.is_sitemap or config.is_feed:\n",
        "            for url in config.urls:\n",
        "                final_urls.extend(config.find_urls(url=url))\n",
        "        else:\n",
        "            final_urls = config.urls\n",
        "\n",
        "        for url in final_urls:\n",
        "            extracted_data = config.extract_url(url=url)\n",
        "            if extracted_data is None:\n",
        "                logger.warning(f\"Unable to crawl {url}, hence skipping it\")\n",
        "                continue\n",
        "            comments = (\n",
        "                \"\" if \"comments\" not in extracted_data else extracted_data[\"comments\"]\n",
        "            )\n",
        "            source_responses.append(\n",
        "                TextPayload(\n",
        "                    processed_text=f\"{extracted_data['text']}. {comments}\",\n",
        "                    meta=extracted_data,\n",
        "                    source_name=self.NAME,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return source_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k93pZF18_HJa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('/content/drive/MyDrive/data_VH/lop5_data_needcrawl.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JZTgwBr8_UKx",
        "outputId": "2625409c-4a47-4f5f-f0ab-18958c6f9712"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e31f9b87-71ef-413d-b55f-901366a5cf9d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tác phẩm</th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thư gửi các em học sinh</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>quang cảnh làng mạc ngày mưa</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nghìn năm văn hiến</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ng...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>màu sắc em yêu</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lòng dân 1</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>lòng dân 2</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>những con sếu bằng giấy</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-nh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>bài ca về trái dất</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>một chuyên gia máy xúc</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ê-mi-li, con…</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>tác phẩm của Si-le và tên phát xít</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>những người bạn tốt</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-nh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>tiếng đàn ba-la-lai-ca trên sông đà</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>kì diệu rừng xanh</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>cái gì quý nhất</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>đất cà mau</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>chuyện một khu vườn nhỏ</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Tiếng vọng</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>mùa thảo quả</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>hành trình của bầy ong</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>người gác rừng tí hon</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ng...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>trồng rừng ngập mặn</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>chuỗi ngọc lam</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>hạt gạo làng ta</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>buồn chư lênh đón cô giáo</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>về ngôi nhà đang xây</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>thầy thuốc như mẹ hiền</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>thầy cúng đi bệnh viện</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>ngụ công xã trịnh tường</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ng...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ca dao về lao động sản xuất</td>\n",
              "      <td>https://baivan.net/content/giai-bai-tap-doc-ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>người công dân số 1</td>\n",
              "      <td>https://baivan.net/content/soan-tieng-viet-5-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>nhà yêu nước nguyễn trung trực</td>\n",
              "      <td>https://baivan.net/content/soan-tieng-viet-5-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>chiếc đồng hồ</td>\n",
              "      <td>https://baivan.net/content/soan-tieng-viet-5-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>người công dân số 1 (tt)</td>\n",
              "      <td>https://baivan.net/content/soan-tieng-viet-5-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Thái sư trần thủ độ</td>\n",
              "      <td>https://baivan.net/content/soan-tieng-viet-5-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>cánh cam lạc mẹ</td>\n",
              "      <td>https://baivan.net/content/soan-tieng-viet-5-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>nhà tài trợ đặc biệt của cách mạng</td>\n",
              "      <td>https://baivan.net/content/soan-tieng-viet-5-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>trí dũng song toàn</td>\n",
              "      <td>https://baivan.net/content/soan-tieng-viet-5-s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e31f9b87-71ef-413d-b55f-901366a5cf9d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e31f9b87-71ef-413d-b55f-901366a5cf9d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e31f9b87-71ef-413d-b55f-901366a5cf9d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e4e42066-a230-4fbc-91cf-5c30d2c7d686\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e4e42066-a230-4fbc-91cf-5c30d2c7d686')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e4e42066-a230-4fbc-91cf-5c30d2c7d686 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                               tác phẩm  \\\n",
              "0               thư gửi các em học sinh   \n",
              "1          quang cảnh làng mạc ngày mưa   \n",
              "2                    nghìn năm văn hiến   \n",
              "3                        màu sắc em yêu   \n",
              "4                            lòng dân 1   \n",
              "5                            lòng dân 2   \n",
              "6               những con sếu bằng giấy   \n",
              "7                    bài ca về trái dất   \n",
              "8                một chuyên gia máy xúc   \n",
              "9                         ê-mi-li, con…   \n",
              "10   tác phẩm của Si-le và tên phát xít   \n",
              "11                  những người bạn tốt   \n",
              "12  tiếng đàn ba-la-lai-ca trên sông đà   \n",
              "13                    kì diệu rừng xanh   \n",
              "14                      cái gì quý nhất   \n",
              "15                           đất cà mau   \n",
              "16              chuyện một khu vườn nhỏ   \n",
              "17                           Tiếng vọng   \n",
              "18                         mùa thảo quả   \n",
              "19               hành trình của bầy ong   \n",
              "20                người gác rừng tí hon   \n",
              "21                  trồng rừng ngập mặn   \n",
              "22                       chuỗi ngọc lam   \n",
              "23                      hạt gạo làng ta   \n",
              "24            buồn chư lênh đón cô giáo   \n",
              "25                 về ngôi nhà đang xây   \n",
              "26               thầy thuốc như mẹ hiền   \n",
              "27               thầy cúng đi bệnh viện   \n",
              "28              ngụ công xã trịnh tường   \n",
              "29          ca dao về lao động sản xuất   \n",
              "30                  người công dân số 1   \n",
              "31       nhà yêu nước nguyễn trung trực   \n",
              "32                        chiếc đồng hồ   \n",
              "33             người công dân số 1 (tt)   \n",
              "34                  Thái sư trần thủ độ   \n",
              "35                      cánh cam lạc mẹ   \n",
              "36   nhà tài trợ đặc biệt của cách mạng   \n",
              "37                   trí dũng song toàn   \n",
              "\n",
              "                                                 link  \n",
              "0   https://baivan.net/content/giai-bai-tap-doc-th...  \n",
              "1   https://baivan.net/content/giai-bai-tap-doc-qu...  \n",
              "2   https://baivan.net/content/giai-bai-tap-doc-ng...  \n",
              "3   https://baivan.net/content/giai-bai-tap-doc-ma...  \n",
              "4   https://baivan.net/content/giai-bai-tap-doc-lo...  \n",
              "5   https://baivan.net/content/giai-bai-tap-doc-lo...  \n",
              "6   https://baivan.net/content/giai-bai-tap-doc-nh...  \n",
              "7   https://baivan.net/content/giai-bai-tap-doc-ba...  \n",
              "8   https://baivan.net/content/giai-bai-tap-doc-mo...  \n",
              "9   https://baivan.net/content/giai-bai-tap-doc-e-...  \n",
              "10  https://baivan.net/content/giai-bai-tap-doc-ta...  \n",
              "11  https://baivan.net/content/giai-bai-tap-doc-nh...  \n",
              "12  https://baivan.net/content/giai-bai-tap-doc-ti...  \n",
              "13  https://baivan.net/content/giai-bai-tap-doc-ki...  \n",
              "14  https://baivan.net/content/giai-bai-tap-doc-ca...  \n",
              "15  https://baivan.net/content/giai-bai-tap-doc-da...  \n",
              "16  https://baivan.net/content/giai-bai-tap-doc-ch...  \n",
              "17  https://baivan.net/content/giai-bai-tap-doc-ti...  \n",
              "18  https://baivan.net/content/giai-bai-tap-doc-mu...  \n",
              "19  https://baivan.net/content/giai-bai-tap-doc-ha...  \n",
              "20  https://baivan.net/content/giai-bai-tap-doc-ng...  \n",
              "21  https://baivan.net/content/giai-bai-tap-doc-tr...  \n",
              "22  https://baivan.net/content/giai-bai-tap-doc-ch...  \n",
              "23  https://baivan.net/content/giai-bai-tap-doc-ha...  \n",
              "24  https://baivan.net/content/giai-bai-tap-doc-bu...  \n",
              "25  https://baivan.net/content/giai-bai-tap-doc-ve...  \n",
              "26  https://baivan.net/content/giai-bai-tap-doc-th...  \n",
              "27  https://baivan.net/content/giai-bai-tap-doc-th...  \n",
              "28  https://baivan.net/content/giai-bai-tap-doc-ng...  \n",
              "29  https://baivan.net/content/giai-bai-tap-doc-ca...  \n",
              "30  https://baivan.net/content/soan-tieng-viet-5-s...  \n",
              "31  https://baivan.net/content/soan-tieng-viet-5-s...  \n",
              "32  https://baivan.net/content/soan-tieng-viet-5-s...  \n",
              "33  https://baivan.net/content/soan-tieng-viet-5-s...  \n",
              "34  https://baivan.net/content/soan-tieng-viet-5-s...  \n",
              "35  https://baivan.net/content/soan-tieng-viet-5-s...  \n",
              "36  https://baivan.net/content/soan-tieng-viet-5-s...  \n",
              "37  https://baivan.net/content/soan-tieng-viet-5-s...  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmQHKmSM_XB2"
      },
      "outputs": [],
      "source": [
        "urls = df['link'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVltceFI5AJL"
      },
      "source": [
        "# Crawl website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntkItpcz5CA6",
        "outputId": "31beef40-b663-4958-ed97-5a6d235dd526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TextPayload(segmented_data={}, meta={'text': '[Crawl là gì](https://cas-solution.vn/crawl/)? Đây là câu hỏi của nhiều Marketer khi mới bắt đầu tìm hiểu về SEO hay phát triển nội dung trên website. Vậy Crawl website hoạt động như nào? Công cụ này đóng vai trò gì trong việc gia tăng thứ hạng, đánh giá điểm chất lượng trên website? Hãy cùng CAS SOLUTION tìm hiểu ngay trong bài viết dưới đây nhé!\\nCrawl là gì?\\nCrawl là gì? Google Crawl (searchbot, spider hay bộ thu thập dữ liệu) là một phần mềm mà Google và các công cụ tìm kiếm khác sử dụng để quét trang Web. Nói một cách đơn giản, nó “thu thập” dữ liệu web từ trang này sang trang khác, tìm kiếm nội dung mới hoặc nội dung cập nhật mà Google chưa có trong cơ sở dữ liệu của mình.\\nSong, bất kỳ công cụ tìm kiếm nào cũng có bộ trình thu thập thông tin riêng. Đối với Google, có hơn 15 loại trình thu thập thông tin khác nhau và trình thu thập thông tin chính của Google được gọi là Googlebot. Googlebot thực hiện cả thu thập thông tin và lập chỉ mục, đó là lý do tại sao chúng ta sẽ xem xét kỹ hơn cách thức hoạt động của nó.\\nVí dụ, Crawl Google search results đóng vai trò quan trọng trong quá trình tìm kiếm, hiểu đơn giản nó hoạt động theo các bước như:\\n- Bạn xuất bản hoặc cập nhật nội dung trên trang web của bạn.\\n- Bot sẽ tìm kiếm và thu thập các trang mới hoặc đã cập nhật trên trang web của bạn.\\n- Google lập chỉ mục các trang mà trình thu thập thông tin tìm thấy.\\n- Google sẽ hiển thị trang của bạn trong kết quả tìm kiếm dựa trên mức độ liên quan của nó với truy vấn của người dùng.\\nTuy nhiên, những công cụ tìm kiếm không phải là nơi duy nhất sử dụng “crawl data from website”. Bạn cũng có thể tự triển khai crawl dữ liệu website để thu thập những thông tin về các trang web.\\nMột số trình thu thập dữ liệu công khai có đôi chút khác biệt so với các trình thu thập dữ liệu của các công cụ tìm kiếm như Google Bot hoặc Bingbot. Nhưng chúng hoạt động theo cách tương tự như các trình thu thập dữ liệu của các công cụ tìm kiếm.\\nVà bạn có thể sử dụng thông tin từ các loại crawl dữ liệu này để cải thiện trang web của mình. Hoặc để hiểu rõ hơn về các trang các web khác.\\nCách thức hoạt động của Crawl là gì?\\nTiếp theo chúng ta sẽ tìm hiểu sâu hơn về cách thức hoạt động thu thập dữ liệu trên Google khi quét 3 yếu tố chính trên một trang web: nội dung, mã và liên kết ngay bên dưới.\\nBằng cách “crawl data from website”, bot có thể đánh giá nội dung của trang. Thông tin này giúp thuật toán Google xác định trang nào có câu trả lời mà người dùng đang tìm kiếm. Đó chính là lý do tại sao việc sử dụng từ khóa SEO phù hợp luôn đóng vai trò quan trọng, giúp cải thiện khả năng kết nối trang đó với các tìm kiếm liên quan của thuật toán.\\nTiếp đó, trong khi đọc nội dung của một trang các spider website cũng thu thập mã HTML của trang đó. Lúc này bạn có thể sử dụng một số mã HTML nhất định (như thẻ meta) để giúp bot crawl data thu thập nội dung, mục đích của trang.\\nCác trình thu thập thông tin cần phải tìm kiếm hàng tỷ trang web. Để thực hiện điều này, chúng đi theo các đường dẫn. Các đường dẫn đó phần lớn được xác định bởi các [liên kết nội bộ](https://cas-solution.vn/internal-link/).\\nNếu website A liên kết đến website B trong nội dung, bot có thể theo dõi liên kết từ website A đến website B. Sau đó xử lý website B.\\nĐây là lý do tại sao liên kết nội bộ lại quan trọng đối với SEO. Nó giúp trình thu thập thông tin của công cụ tìm kiếm tìm và lập chỉ mục tất cả các trang trên trang web.\\nTầm quan trọng của crawl dữ liệu trong SEO\\nSau khi hiểu được định nghĩa và cách thức hoạt động, các marketer hoặc người làm SEO nên biết được tầm quan trọng của crawl là gì để có thể có định hướng phát triển [chiến lược SEO](https://cas-solution.vn/chien-luoc-seo/) tốt nhất cho website của mình. Quá trình crawl dữ liệu giúp:\\nĐiều hướng website của bạn đi đúng hướng phát triển\\nCrawl data from website có thể cho bạn biết chính xác mức độ dễ dàng để bot Google điều hướng và xử lý nội dung trên website. Ví dụ, nhờ vào việc quét website bạn có thể tìm ra được các vấn đề ngăn chặn việc crawl dữ liệu hiệu quả như chuyển hướng tạm thời, nội dung trùng lặp,…\\nBên cạnh đó, việc crawl dữ liệu có thể giúp bạn sớm phát hiện ra những trang mà Google không thể lập chỉ mục để tìm ra nguyên nhân, cách khắc phục. Điều này giúp tránh nguy cơ mất thời gian, tiền bạc và ảnh hưởng đến thứ hạng SEO của web.\\nXác định các liên kết bị hỏng để cải thiện sức khỏe của trang web và giá trị liên kết\\n[Liên kết hỏng](https://cas-solution.vn/loi-404-la-gi/) là một trong những lỗi phổ biến nhất. Chúng gây phiền toái cho người theo dõi bài viết, hay muốn tìm hiểu một nội dung nào đó trên website. Để crawl dữ liệu qua trình thu thập dữ liệu web Google là một cách thức tuyệt vời để rà soát các lỗi này, giúp tìm ra nhanh chóng các liên kết bị hỏng và sửa lại để đảm bảo website hoạt động ổn định.\\nSong, khi phát hiện các liên kết hỏng bạn có thể khắc phục bằng cách: xóa liên kết, thay thế liên kết hoặc báo cáo sự cố với chủ sở hữu trang web liên kết nếu đó là liên kết ngoài.\\nTìm nội dung trùng lặp trên hệ thống nội dung hiển thị\\nNội dung trùng lặp là nội dung giống hệt hoặc gần giống nhau. Khi sử dụng công cụ crawl google search results kiểm tra trang web có thể giúp bạn tìm ra [nội dung trùng lặp](https://cas-solution.vn/duplicate-content/).\\nNhững nội dung này có thể gây ra các vấn đề SEO nghiêm trọng, gây nhầm lẫn cho các công cụ tìm kiếm. Nó cũng có thể khiến phiên bản trang không đúng hiển thị trong kết quả tìm kiếm hoặc thậm chí bộ máy đọc của Google có thể nhầm lẫn trong việc bạn có đang sử dụng các hoạt động thao túng gian dối hay không.\\nSau khi xác định được nội dung cần chỉnh sửa, bạn có thể chỉnh sửa lại để phù hợp hơn với kết quả tìm kiếm.\\nCác yếu tố ảnh hưởng đến việc ra quyết định crawl data from website của Google\\nKhông có lịch trình cụ thể về thời điểm Google thực hiện crawl data from website của bạn. Điều này có thể ví như một cuộc ghé thăm ngẫu nhiên khi bộ máy của Google “lê la” khắp khu vực chúng hoạt động.\\nTuy nhiên, một số yếu tố có thể ảnh hưởng đến tần suất truy cập, rà soát của Google bao gồm:\\n- Kích thước trang web lớn, có nhiều trang, hệ thống nội dung, thông tin lớn và có lượng truy cập thường xuyên cao.\\n- Mức độ phổ biến của trang: Nếu trang web của bạn có số lượng liên kết inbound cao từ các nguồn uy tín, Google sẽ coi đây là tín hiệu của một tài nguyên chất lượng, có khả năng dẫn đến việc thu thập dữ liệu thường xuyên hơn.\\n[Tốc độ của website](https://cas-solution.vn/toc-do-website/): Một trang web tải nhanh sẽ dễ dàng hơn cho Googlebot thu thập dữ liệu.- Độ mới, sáng tạo của nội dung: Nếu bạn liên tục cập nhật nội dung mới, có liên quan trên trang web của mình thì Google có khả năng sẽ ghé thăm thường xuyên hơn để cập nhật chỉ mục.\\nMột số lỗi thường gặp khi “crawl data from website”\\nNgoài việc tìm hiểu crawl là gì, cách thức hoạt động, tầm quan trọng của việc quét dữ liệu thì trong bài viết này CAS Solution sẽ cùng bạn tìm hiểu về một số lỗi thường gặp khi trình thu thập dữ liệu của công cụ tìm kiếm không thể điều hướng qua các trang web theo cách thông thường. Một số lỗi phổ biến như:\\n- Bị chặn bởi\\n[robots.txt](https://cas-solution.vn/robots-txt/): Website hoặc một số trang con trên trang web có thể bị chặn bởi một lệnh trong tệp Robots.txt. Điều này ngăn chặn Googlebot hoặc các công cụ tìm kiếm khác thu thập thông tin các trang đó. - Lỗi Error 404: Trang trả về lỗi 404 không thể thu thập dữ liệu. Lỗi này có thể xảy ra khi các trang bị xóa hoặc URL của chúng được thay đổi mà không có hướng dẫn phù hợp.\\n- Lỗi máy chủ: Nếu máy chủ thường xuyên hoạt động liên tục hoặc phản hồi chậm, Googlebot có thể gặp sự cố khi thu thập trang web dữ liệu của bạn.\\n- Trang web quá chậm: Nếu trang web tải quá chậm, Googlebot có thể bỏ cuộc trước khi thu thập dữ liệu xong. Điều này chủ yếu áp xuất hiện khi trang web chậm một cách khó hiểu.\\n- Lạm dụng thẻ meta: Việc sử dụng thẻ meta noindex hoặc nofollow không chính xác cũng có thể ngăn chặn Google thu thập dữ liệu trang web của bạn.\\nLàm thế nào để tăng khả năng thu thập thông tin?\\nNói chung, để website hoạt động và tăng trưởng bền bỉ, người vận hành web cần phải làm cho crawl dữ liệu từ công cụ tìm kiếm càng nhiều càng tốt. Dưới đây là một số điều có thể thực hiện để tăng khả năng thu thập dữ liệu của trang web như:\\n- Tạo ra nội dung chất lượng cao được cập nhật thường xuyên\\n- Cải thiện tốc độ hoạt động của trang web nhanh nhất có thể\\n- Kiểm tra các liên kết nội bộ để điều hướng tốt hơn\\n- Sử dụng các cụm từ khóa trọng tâm, tối ưu hóa hình ảnh và văn bản thay thế\\n- Tạo và gửi sitemap.xml để đảm bảo rằng trình thu thập thông tin không bị nhầm lẫn, bỏ sót việc lập chỉ mục các trang quan trọng.\\n- Thiết kế trang web của bạn thân thiện với thiết bị di động\\nTóm lại, trình thu thập dữ liệu chính của Google, Googlebot, hoạt động theo các thuật toán phức tạp, nhưng bạn vẫn có thể “điều hướng” hành vi của nó để làm cho nó có lợi cho trang web của bạn. Bên cạnh đó, hầu hết các bước tối ưu hóa quy trình thu thập dữ liệu đều lặp lại các bước của SEO chuẩn mà chúng ta đều quen thuộc.\\nTrên đây, những thông tin giải đáp về crawl là gì và một số khía cạnh của việc crawl dữ liệu trên các công cụ tìm kiếm. Cảm ơn bạn đã dành thời gian tham khảo bài viết của CAS SOLUTION. Để được tư vấn về [dịch vụ chăm sóc website](https://cas-solution.vn/phat-trien-website/), vui lòng liên hệ tới hotline để được hỗ trợ nhanh nhất. Chúc bạn luôn thành công trong lĩnh vực kinh doanh của mình!', 'comments': ''}, source_name='Crawler', processed_text='[Crawl là gì](https://cas-solution.vn/crawl/)? Đây là câu hỏi của nhiều Marketer khi mới bắt đầu tìm hiểu về SEO hay phát triển nội dung trên website. Vậy Crawl website hoạt động như nào? Công cụ này đóng vai trò gì trong việc gia tăng thứ hạng, đánh giá điểm chất lượng trên website? Hãy cùng CAS SOLUTION tìm hiểu ngay trong bài viết dưới đây nhé!\\nCrawl là gì?\\nCrawl là gì? Google Crawl (searchbot, spider hay bộ thu thập dữ liệu) là một phần mềm mà Google và các công cụ tìm kiếm khác sử dụng để quét trang Web. Nói một cách đơn giản, nó “thu thập” dữ liệu web từ trang này sang trang khác, tìm kiếm nội dung mới hoặc nội dung cập nhật mà Google chưa có trong cơ sở dữ liệu của mình.\\nSong, bất kỳ công cụ tìm kiếm nào cũng có bộ trình thu thập thông tin riêng. Đối với Google, có hơn 15 loại trình thu thập thông tin khác nhau và trình thu thập thông tin chính của Google được gọi là Googlebot. Googlebot thực hiện cả thu thập thông tin và lập chỉ mục, đó là lý do tại sao chúng ta sẽ xem xét kỹ hơn cách thức hoạt động của nó.\\nVí dụ, Crawl Google search results đóng vai trò quan trọng trong quá trình tìm kiếm, hiểu đơn giản nó hoạt động theo các bước như:\\n- Bạn xuất bản hoặc cập nhật nội dung trên trang web của bạn.\\n- Bot sẽ tìm kiếm và thu thập các trang mới hoặc đã cập nhật trên trang web của bạn.\\n- Google lập chỉ mục các trang mà trình thu thập thông tin tìm thấy.\\n- Google sẽ hiển thị trang của bạn trong kết quả tìm kiếm dựa trên mức độ liên quan của nó với truy vấn của người dùng.\\nTuy nhiên, những công cụ tìm kiếm không phải là nơi duy nhất sử dụng “crawl data from website”. Bạn cũng có thể tự triển khai crawl dữ liệu website để thu thập những thông tin về các trang web.\\nMột số trình thu thập dữ liệu công khai có đôi chút khác biệt so với các trình thu thập dữ liệu của các công cụ tìm kiếm như Google Bot hoặc Bingbot. Nhưng chúng hoạt động theo cách tương tự như các trình thu thập dữ liệu của các công cụ tìm kiếm.\\nVà bạn có thể sử dụng thông tin từ các loại crawl dữ liệu này để cải thiện trang web của mình. Hoặc để hiểu rõ hơn về các trang các web khác.\\nCách thức hoạt động của Crawl là gì?\\nTiếp theo chúng ta sẽ tìm hiểu sâu hơn về cách thức hoạt động thu thập dữ liệu trên Google khi quét 3 yếu tố chính trên một trang web: nội dung, mã và liên kết ngay bên dưới.\\nBằng cách “crawl data from website”, bot có thể đánh giá nội dung của trang. Thông tin này giúp thuật toán Google xác định trang nào có câu trả lời mà người dùng đang tìm kiếm. Đó chính là lý do tại sao việc sử dụng từ khóa SEO phù hợp luôn đóng vai trò quan trọng, giúp cải thiện khả năng kết nối trang đó với các tìm kiếm liên quan của thuật toán.\\nTiếp đó, trong khi đọc nội dung của một trang các spider website cũng thu thập mã HTML của trang đó. Lúc này bạn có thể sử dụng một số mã HTML nhất định (như thẻ meta) để giúp bot crawl data thu thập nội dung, mục đích của trang.\\nCác trình thu thập thông tin cần phải tìm kiếm hàng tỷ trang web. Để thực hiện điều này, chúng đi theo các đường dẫn. Các đường dẫn đó phần lớn được xác định bởi các [liên kết nội bộ](https://cas-solution.vn/internal-link/).\\nNếu website A liên kết đến website B trong nội dung, bot có thể theo dõi liên kết từ website A đến website B. Sau đó xử lý website B.\\nĐây là lý do tại sao liên kết nội bộ lại quan trọng đối với SEO. Nó giúp trình thu thập thông tin của công cụ tìm kiếm tìm và lập chỉ mục tất cả các trang trên trang web.\\nTầm quan trọng của crawl dữ liệu trong SEO\\nSau khi hiểu được định nghĩa và cách thức hoạt động, các marketer hoặc người làm SEO nên biết được tầm quan trọng của crawl là gì để có thể có định hướng phát triển [chiến lược SEO](https://cas-solution.vn/chien-luoc-seo/) tốt nhất cho website của mình. Quá trình crawl dữ liệu giúp:\\nĐiều hướng website của bạn đi đúng hướng phát triển\\nCrawl data from website có thể cho bạn biết chính xác mức độ dễ dàng để bot Google điều hướng và xử lý nội dung trên website. Ví dụ, nhờ vào việc quét website bạn có thể tìm ra được các vấn đề ngăn chặn việc crawl dữ liệu hiệu quả như chuyển hướng tạm thời, nội dung trùng lặp,…\\nBên cạnh đó, việc crawl dữ liệu có thể giúp bạn sớm phát hiện ra những trang mà Google không thể lập chỉ mục để tìm ra nguyên nhân, cách khắc phục. Điều này giúp tránh nguy cơ mất thời gian, tiền bạc và ảnh hưởng đến thứ hạng SEO của web.\\nXác định các liên kết bị hỏng để cải thiện sức khỏe của trang web và giá trị liên kết\\n[Liên kết hỏng](https://cas-solution.vn/loi-404-la-gi/) là một trong những lỗi phổ biến nhất. Chúng gây phiền toái cho người theo dõi bài viết, hay muốn tìm hiểu một nội dung nào đó trên website. Để crawl dữ liệu qua trình thu thập dữ liệu web Google là một cách thức tuyệt vời để rà soát các lỗi này, giúp tìm ra nhanh chóng các liên kết bị hỏng và sửa lại để đảm bảo website hoạt động ổn định.\\nSong, khi phát hiện các liên kết hỏng bạn có thể khắc phục bằng cách: xóa liên kết, thay thế liên kết hoặc báo cáo sự cố với chủ sở hữu trang web liên kết nếu đó là liên kết ngoài.\\nTìm nội dung trùng lặp trên hệ thống nội dung hiển thị\\nNội dung trùng lặp là nội dung giống hệt hoặc gần giống nhau. Khi sử dụng công cụ crawl google search results kiểm tra trang web có thể giúp bạn tìm ra [nội dung trùng lặp](https://cas-solution.vn/duplicate-content/).\\nNhững nội dung này có thể gây ra các vấn đề SEO nghiêm trọng, gây nhầm lẫn cho các công cụ tìm kiếm. Nó cũng có thể khiến phiên bản trang không đúng hiển thị trong kết quả tìm kiếm hoặc thậm chí bộ máy đọc của Google có thể nhầm lẫn trong việc bạn có đang sử dụng các hoạt động thao túng gian dối hay không.\\nSau khi xác định được nội dung cần chỉnh sửa, bạn có thể chỉnh sửa lại để phù hợp hơn với kết quả tìm kiếm.\\nCác yếu tố ảnh hưởng đến việc ra quyết định crawl data from website của Google\\nKhông có lịch trình cụ thể về thời điểm Google thực hiện crawl data from website của bạn. Điều này có thể ví như một cuộc ghé thăm ngẫu nhiên khi bộ máy của Google “lê la” khắp khu vực chúng hoạt động.\\nTuy nhiên, một số yếu tố có thể ảnh hưởng đến tần suất truy cập, rà soát của Google bao gồm:\\n- Kích thước trang web lớn, có nhiều trang, hệ thống nội dung, thông tin lớn và có lượng truy cập thường xuyên cao.\\n- Mức độ phổ biến của trang: Nếu trang web của bạn có số lượng liên kết inbound cao từ các nguồn uy tín, Google sẽ coi đây là tín hiệu của một tài nguyên chất lượng, có khả năng dẫn đến việc thu thập dữ liệu thường xuyên hơn.\\n[Tốc độ của website](https://cas-solution.vn/toc-do-website/): Một trang web tải nhanh sẽ dễ dàng hơn cho Googlebot thu thập dữ liệu.- Độ mới, sáng tạo của nội dung: Nếu bạn liên tục cập nhật nội dung mới, có liên quan trên trang web của mình thì Google có khả năng sẽ ghé thăm thường xuyên hơn để cập nhật chỉ mục.\\nMột số lỗi thường gặp khi “crawl data from website”\\nNgoài việc tìm hiểu crawl là gì, cách thức hoạt động, tầm quan trọng của việc quét dữ liệu thì trong bài viết này CAS Solution sẽ cùng bạn tìm hiểu về một số lỗi thường gặp khi trình thu thập dữ liệu của công cụ tìm kiếm không thể điều hướng qua các trang web theo cách thông thường. Một số lỗi phổ biến như:\\n- Bị chặn bởi\\n[robots.txt](https://cas-solution.vn/robots-txt/): Website hoặc một số trang con trên trang web có thể bị chặn bởi một lệnh trong tệp Robots.txt. Điều này ngăn chặn Googlebot hoặc các công cụ tìm kiếm khác thu thập thông tin các trang đó. - Lỗi Error 404: Trang trả về lỗi 404 không thể thu thập dữ liệu. Lỗi này có thể xảy ra khi các trang bị xóa hoặc URL của chúng được thay đổi mà không có hướng dẫn phù hợp.\\n- Lỗi máy chủ: Nếu máy chủ thường xuyên hoạt động liên tục hoặc phản hồi chậm, Googlebot có thể gặp sự cố khi thu thập trang web dữ liệu của bạn.\\n- Trang web quá chậm: Nếu trang web tải quá chậm, Googlebot có thể bỏ cuộc trước khi thu thập dữ liệu xong. Điều này chủ yếu áp xuất hiện khi trang web chậm một cách khó hiểu.\\n- Lạm dụng thẻ meta: Việc sử dụng thẻ meta noindex hoặc nofollow không chính xác cũng có thể ngăn chặn Google thu thập dữ liệu trang web của bạn.\\nLàm thế nào để tăng khả năng thu thập thông tin?\\nNói chung, để website hoạt động và tăng trưởng bền bỉ, người vận hành web cần phải làm cho crawl dữ liệu từ công cụ tìm kiếm càng nhiều càng tốt. Dưới đây là một số điều có thể thực hiện để tăng khả năng thu thập dữ liệu của trang web như:\\n- Tạo ra nội dung chất lượng cao được cập nhật thường xuyên\\n- Cải thiện tốc độ hoạt động của trang web nhanh nhất có thể\\n- Kiểm tra các liên kết nội bộ để điều hướng tốt hơn\\n- Sử dụng các cụm từ khóa trọng tâm, tối ưu hóa hình ảnh và văn bản thay thế\\n- Tạo và gửi sitemap.xml để đảm bảo rằng trình thu thập thông tin không bị nhầm lẫn, bỏ sót việc lập chỉ mục các trang quan trọng.\\n- Thiết kế trang web của bạn thân thiện với thiết bị di động\\nTóm lại, trình thu thập dữ liệu chính của Google, Googlebot, hoạt động theo các thuật toán phức tạp, nhưng bạn vẫn có thể “điều hướng” hành vi của nó để làm cho nó có lợi cho trang web của bạn. Bên cạnh đó, hầu hết các bước tối ưu hóa quy trình thu thập dữ liệu đều lặp lại các bước của SEO chuẩn mà chúng ta đều quen thuộc.\\nTrên đây, những thông tin giải đáp về crawl là gì và một số khía cạnh của việc crawl dữ liệu trên các công cụ tìm kiếm. Cảm ơn bạn đã dành thời gian tham khảo bài viết của CAS SOLUTION. Để được tư vấn về [dịch vụ chăm sóc website](https://cas-solution.vn/phat-trien-website/), vui lòng liên hệ tới hotline để được hỗ trợ nhanh nhất. Chúc bạn luôn thành công trong lĩnh vực kinh doanh của mình!. ')]\n",
            "0\n",
            "[{'segmented_data': {}, 'meta': {'text': '[Crawl là gì](https://cas-solution.vn/crawl/)? Đây là câu hỏi của nhiều Marketer khi mới bắt đầu tìm hiểu về SEO hay phát triển nội dung trên website. Vậy Crawl website hoạt động như nào? Công cụ này đóng vai trò gì trong việc gia tăng thứ hạng, đánh giá điểm chất lượng trên website? Hãy cùng CAS SOLUTION tìm hiểu ngay trong bài viết dưới đây nhé!\\nCrawl là gì?\\nCrawl là gì? Google Crawl (searchbot, spider hay bộ thu thập dữ liệu) là một phần mềm mà Google và các công cụ tìm kiếm khác sử dụng để quét trang Web. Nói một cách đơn giản, nó “thu thập” dữ liệu web từ trang này sang trang khác, tìm kiếm nội dung mới hoặc nội dung cập nhật mà Google chưa có trong cơ sở dữ liệu của mình.\\nSong, bất kỳ công cụ tìm kiếm nào cũng có bộ trình thu thập thông tin riêng. Đối với Google, có hơn 15 loại trình thu thập thông tin khác nhau và trình thu thập thông tin chính của Google được gọi là Googlebot. Googlebot thực hiện cả thu thập thông tin và lập chỉ mục, đó là lý do tại sao chúng ta sẽ xem xét kỹ hơn cách thức hoạt động của nó.\\nVí dụ, Crawl Google search results đóng vai trò quan trọng trong quá trình tìm kiếm, hiểu đơn giản nó hoạt động theo các bước như:\\n- Bạn xuất bản hoặc cập nhật nội dung trên trang web của bạn.\\n- Bot sẽ tìm kiếm và thu thập các trang mới hoặc đã cập nhật trên trang web của bạn.\\n- Google lập chỉ mục các trang mà trình thu thập thông tin tìm thấy.\\n- Google sẽ hiển thị trang của bạn trong kết quả tìm kiếm dựa trên mức độ liên quan của nó với truy vấn của người dùng.\\nTuy nhiên, những công cụ tìm kiếm không phải là nơi duy nhất sử dụng “crawl data from website”. Bạn cũng có thể tự triển khai crawl dữ liệu website để thu thập những thông tin về các trang web.\\nMột số trình thu thập dữ liệu công khai có đôi chút khác biệt so với các trình thu thập dữ liệu của các công cụ tìm kiếm như Google Bot hoặc Bingbot. Nhưng chúng hoạt động theo cách tương tự như các trình thu thập dữ liệu của các công cụ tìm kiếm.\\nVà bạn có thể sử dụng thông tin từ các loại crawl dữ liệu này để cải thiện trang web của mình. Hoặc để hiểu rõ hơn về các trang các web khác.\\nCách thức hoạt động của Crawl là gì?\\nTiếp theo chúng ta sẽ tìm hiểu sâu hơn về cách thức hoạt động thu thập dữ liệu trên Google khi quét 3 yếu tố chính trên một trang web: nội dung, mã và liên kết ngay bên dưới.\\nBằng cách “crawl data from website”, bot có thể đánh giá nội dung của trang. Thông tin này giúp thuật toán Google xác định trang nào có câu trả lời mà người dùng đang tìm kiếm. Đó chính là lý do tại sao việc sử dụng từ khóa SEO phù hợp luôn đóng vai trò quan trọng, giúp cải thiện khả năng kết nối trang đó với các tìm kiếm liên quan của thuật toán.\\nTiếp đó, trong khi đọc nội dung của một trang các spider website cũng thu thập mã HTML của trang đó. Lúc này bạn có thể sử dụng một số mã HTML nhất định (như thẻ meta) để giúp bot crawl data thu thập nội dung, mục đích của trang.\\nCác trình thu thập thông tin cần phải tìm kiếm hàng tỷ trang web. Để thực hiện điều này, chúng đi theo các đường dẫn. Các đường dẫn đó phần lớn được xác định bởi các [liên kết nội bộ](https://cas-solution.vn/internal-link/).\\nNếu website A liên kết đến website B trong nội dung, bot có thể theo dõi liên kết từ website A đến website B. Sau đó xử lý website B.\\nĐây là lý do tại sao liên kết nội bộ lại quan trọng đối với SEO. Nó giúp trình thu thập thông tin của công cụ tìm kiếm tìm và lập chỉ mục tất cả các trang trên trang web.\\nTầm quan trọng của crawl dữ liệu trong SEO\\nSau khi hiểu được định nghĩa và cách thức hoạt động, các marketer hoặc người làm SEO nên biết được tầm quan trọng của crawl là gì để có thể có định hướng phát triển [chiến lược SEO](https://cas-solution.vn/chien-luoc-seo/) tốt nhất cho website của mình. Quá trình crawl dữ liệu giúp:\\nĐiều hướng website của bạn đi đúng hướng phát triển\\nCrawl data from website có thể cho bạn biết chính xác mức độ dễ dàng để bot Google điều hướng và xử lý nội dung trên website. Ví dụ, nhờ vào việc quét website bạn có thể tìm ra được các vấn đề ngăn chặn việc crawl dữ liệu hiệu quả như chuyển hướng tạm thời, nội dung trùng lặp,…\\nBên cạnh đó, việc crawl dữ liệu có thể giúp bạn sớm phát hiện ra những trang mà Google không thể lập chỉ mục để tìm ra nguyên nhân, cách khắc phục. Điều này giúp tránh nguy cơ mất thời gian, tiền bạc và ảnh hưởng đến thứ hạng SEO của web.\\nXác định các liên kết bị hỏng để cải thiện sức khỏe của trang web và giá trị liên kết\\n[Liên kết hỏng](https://cas-solution.vn/loi-404-la-gi/) là một trong những lỗi phổ biến nhất. Chúng gây phiền toái cho người theo dõi bài viết, hay muốn tìm hiểu một nội dung nào đó trên website. Để crawl dữ liệu qua trình thu thập dữ liệu web Google là một cách thức tuyệt vời để rà soát các lỗi này, giúp tìm ra nhanh chóng các liên kết bị hỏng và sửa lại để đảm bảo website hoạt động ổn định.\\nSong, khi phát hiện các liên kết hỏng bạn có thể khắc phục bằng cách: xóa liên kết, thay thế liên kết hoặc báo cáo sự cố với chủ sở hữu trang web liên kết nếu đó là liên kết ngoài.\\nTìm nội dung trùng lặp trên hệ thống nội dung hiển thị\\nNội dung trùng lặp là nội dung giống hệt hoặc gần giống nhau. Khi sử dụng công cụ crawl google search results kiểm tra trang web có thể giúp bạn tìm ra [nội dung trùng lặp](https://cas-solution.vn/duplicate-content/).\\nNhững nội dung này có thể gây ra các vấn đề SEO nghiêm trọng, gây nhầm lẫn cho các công cụ tìm kiếm. Nó cũng có thể khiến phiên bản trang không đúng hiển thị trong kết quả tìm kiếm hoặc thậm chí bộ máy đọc của Google có thể nhầm lẫn trong việc bạn có đang sử dụng các hoạt động thao túng gian dối hay không.\\nSau khi xác định được nội dung cần chỉnh sửa, bạn có thể chỉnh sửa lại để phù hợp hơn với kết quả tìm kiếm.\\nCác yếu tố ảnh hưởng đến việc ra quyết định crawl data from website của Google\\nKhông có lịch trình cụ thể về thời điểm Google thực hiện crawl data from website của bạn. Điều này có thể ví như một cuộc ghé thăm ngẫu nhiên khi bộ máy của Google “lê la” khắp khu vực chúng hoạt động.\\nTuy nhiên, một số yếu tố có thể ảnh hưởng đến tần suất truy cập, rà soát của Google bao gồm:\\n- Kích thước trang web lớn, có nhiều trang, hệ thống nội dung, thông tin lớn và có lượng truy cập thường xuyên cao.\\n- Mức độ phổ biến của trang: Nếu trang web của bạn có số lượng liên kết inbound cao từ các nguồn uy tín, Google sẽ coi đây là tín hiệu của một tài nguyên chất lượng, có khả năng dẫn đến việc thu thập dữ liệu thường xuyên hơn.\\n[Tốc độ của website](https://cas-solution.vn/toc-do-website/): Một trang web tải nhanh sẽ dễ dàng hơn cho Googlebot thu thập dữ liệu.- Độ mới, sáng tạo của nội dung: Nếu bạn liên tục cập nhật nội dung mới, có liên quan trên trang web của mình thì Google có khả năng sẽ ghé thăm thường xuyên hơn để cập nhật chỉ mục.\\nMột số lỗi thường gặp khi “crawl data from website”\\nNgoài việc tìm hiểu crawl là gì, cách thức hoạt động, tầm quan trọng của việc quét dữ liệu thì trong bài viết này CAS Solution sẽ cùng bạn tìm hiểu về một số lỗi thường gặp khi trình thu thập dữ liệu của công cụ tìm kiếm không thể điều hướng qua các trang web theo cách thông thường. Một số lỗi phổ biến như:\\n- Bị chặn bởi\\n[robots.txt](https://cas-solution.vn/robots-txt/): Website hoặc một số trang con trên trang web có thể bị chặn bởi một lệnh trong tệp Robots.txt. Điều này ngăn chặn Googlebot hoặc các công cụ tìm kiếm khác thu thập thông tin các trang đó. - Lỗi Error 404: Trang trả về lỗi 404 không thể thu thập dữ liệu. Lỗi này có thể xảy ra khi các trang bị xóa hoặc URL của chúng được thay đổi mà không có hướng dẫn phù hợp.\\n- Lỗi máy chủ: Nếu máy chủ thường xuyên hoạt động liên tục hoặc phản hồi chậm, Googlebot có thể gặp sự cố khi thu thập trang web dữ liệu của bạn.\\n- Trang web quá chậm: Nếu trang web tải quá chậm, Googlebot có thể bỏ cuộc trước khi thu thập dữ liệu xong. Điều này chủ yếu áp xuất hiện khi trang web chậm một cách khó hiểu.\\n- Lạm dụng thẻ meta: Việc sử dụng thẻ meta noindex hoặc nofollow không chính xác cũng có thể ngăn chặn Google thu thập dữ liệu trang web của bạn.\\nLàm thế nào để tăng khả năng thu thập thông tin?\\nNói chung, để website hoạt động và tăng trưởng bền bỉ, người vận hành web cần phải làm cho crawl dữ liệu từ công cụ tìm kiếm càng nhiều càng tốt. Dưới đây là một số điều có thể thực hiện để tăng khả năng thu thập dữ liệu của trang web như:\\n- Tạo ra nội dung chất lượng cao được cập nhật thường xuyên\\n- Cải thiện tốc độ hoạt động của trang web nhanh nhất có thể\\n- Kiểm tra các liên kết nội bộ để điều hướng tốt hơn\\n- Sử dụng các cụm từ khóa trọng tâm, tối ưu hóa hình ảnh và văn bản thay thế\\n- Tạo và gửi sitemap.xml để đảm bảo rằng trình thu thập thông tin không bị nhầm lẫn, bỏ sót việc lập chỉ mục các trang quan trọng.\\n- Thiết kế trang web của bạn thân thiện với thiết bị di động\\nTóm lại, trình thu thập dữ liệu chính của Google, Googlebot, hoạt động theo các thuật toán phức tạp, nhưng bạn vẫn có thể “điều hướng” hành vi của nó để làm cho nó có lợi cho trang web của bạn. Bên cạnh đó, hầu hết các bước tối ưu hóa quy trình thu thập dữ liệu đều lặp lại các bước của SEO chuẩn mà chúng ta đều quen thuộc.\\nTrên đây, những thông tin giải đáp về crawl là gì và một số khía cạnh của việc crawl dữ liệu trên các công cụ tìm kiếm. Cảm ơn bạn đã dành thời gian tham khảo bài viết của CAS SOLUTION. Để được tư vấn về [dịch vụ chăm sóc website](https://cas-solution.vn/phat-trien-website/), vui lòng liên hệ tới hotline để được hỗ trợ nhanh nhất. Chúc bạn luôn thành công trong lĩnh vực kinh doanh của mình!', 'comments': ''}, 'source_name': 'Crawler', 'processed_text': '[Crawl là gì](https://cas-solution.vn/crawl/)? Đây là câu hỏi của nhiều Marketer khi mới bắt đầu tìm hiểu về SEO hay phát triển nội dung trên website. Vậy Crawl website hoạt động như nào? Công cụ này đóng vai trò gì trong việc gia tăng thứ hạng, đánh giá điểm chất lượng trên website? Hãy cùng CAS SOLUTION tìm hiểu ngay trong bài viết dưới đây nhé!\\nCrawl là gì?\\nCrawl là gì? Google Crawl (searchbot, spider hay bộ thu thập dữ liệu) là một phần mềm mà Google và các công cụ tìm kiếm khác sử dụng để quét trang Web. Nói một cách đơn giản, nó “thu thập” dữ liệu web từ trang này sang trang khác, tìm kiếm nội dung mới hoặc nội dung cập nhật mà Google chưa có trong cơ sở dữ liệu của mình.\\nSong, bất kỳ công cụ tìm kiếm nào cũng có bộ trình thu thập thông tin riêng. Đối với Google, có hơn 15 loại trình thu thập thông tin khác nhau và trình thu thập thông tin chính của Google được gọi là Googlebot. Googlebot thực hiện cả thu thập thông tin và lập chỉ mục, đó là lý do tại sao chúng ta sẽ xem xét kỹ hơn cách thức hoạt động của nó.\\nVí dụ, Crawl Google search results đóng vai trò quan trọng trong quá trình tìm kiếm, hiểu đơn giản nó hoạt động theo các bước như:\\n- Bạn xuất bản hoặc cập nhật nội dung trên trang web của bạn.\\n- Bot sẽ tìm kiếm và thu thập các trang mới hoặc đã cập nhật trên trang web của bạn.\\n- Google lập chỉ mục các trang mà trình thu thập thông tin tìm thấy.\\n- Google sẽ hiển thị trang của bạn trong kết quả tìm kiếm dựa trên mức độ liên quan của nó với truy vấn của người dùng.\\nTuy nhiên, những công cụ tìm kiếm không phải là nơi duy nhất sử dụng “crawl data from website”. Bạn cũng có thể tự triển khai crawl dữ liệu website để thu thập những thông tin về các trang web.\\nMột số trình thu thập dữ liệu công khai có đôi chút khác biệt so với các trình thu thập dữ liệu của các công cụ tìm kiếm như Google Bot hoặc Bingbot. Nhưng chúng hoạt động theo cách tương tự như các trình thu thập dữ liệu của các công cụ tìm kiếm.\\nVà bạn có thể sử dụng thông tin từ các loại crawl dữ liệu này để cải thiện trang web của mình. Hoặc để hiểu rõ hơn về các trang các web khác.\\nCách thức hoạt động của Crawl là gì?\\nTiếp theo chúng ta sẽ tìm hiểu sâu hơn về cách thức hoạt động thu thập dữ liệu trên Google khi quét 3 yếu tố chính trên một trang web: nội dung, mã và liên kết ngay bên dưới.\\nBằng cách “crawl data from website”, bot có thể đánh giá nội dung của trang. Thông tin này giúp thuật toán Google xác định trang nào có câu trả lời mà người dùng đang tìm kiếm. Đó chính là lý do tại sao việc sử dụng từ khóa SEO phù hợp luôn đóng vai trò quan trọng, giúp cải thiện khả năng kết nối trang đó với các tìm kiếm liên quan của thuật toán.\\nTiếp đó, trong khi đọc nội dung của một trang các spider website cũng thu thập mã HTML của trang đó. Lúc này bạn có thể sử dụng một số mã HTML nhất định (như thẻ meta) để giúp bot crawl data thu thập nội dung, mục đích của trang.\\nCác trình thu thập thông tin cần phải tìm kiếm hàng tỷ trang web. Để thực hiện điều này, chúng đi theo các đường dẫn. Các đường dẫn đó phần lớn được xác định bởi các [liên kết nội bộ](https://cas-solution.vn/internal-link/).\\nNếu website A liên kết đến website B trong nội dung, bot có thể theo dõi liên kết từ website A đến website B. Sau đó xử lý website B.\\nĐây là lý do tại sao liên kết nội bộ lại quan trọng đối với SEO. Nó giúp trình thu thập thông tin của công cụ tìm kiếm tìm và lập chỉ mục tất cả các trang trên trang web.\\nTầm quan trọng của crawl dữ liệu trong SEO\\nSau khi hiểu được định nghĩa và cách thức hoạt động, các marketer hoặc người làm SEO nên biết được tầm quan trọng của crawl là gì để có thể có định hướng phát triển [chiến lược SEO](https://cas-solution.vn/chien-luoc-seo/) tốt nhất cho website của mình. Quá trình crawl dữ liệu giúp:\\nĐiều hướng website của bạn đi đúng hướng phát triển\\nCrawl data from website có thể cho bạn biết chính xác mức độ dễ dàng để bot Google điều hướng và xử lý nội dung trên website. Ví dụ, nhờ vào việc quét website bạn có thể tìm ra được các vấn đề ngăn chặn việc crawl dữ liệu hiệu quả như chuyển hướng tạm thời, nội dung trùng lặp,…\\nBên cạnh đó, việc crawl dữ liệu có thể giúp bạn sớm phát hiện ra những trang mà Google không thể lập chỉ mục để tìm ra nguyên nhân, cách khắc phục. Điều này giúp tránh nguy cơ mất thời gian, tiền bạc và ảnh hưởng đến thứ hạng SEO của web.\\nXác định các liên kết bị hỏng để cải thiện sức khỏe của trang web và giá trị liên kết\\n[Liên kết hỏng](https://cas-solution.vn/loi-404-la-gi/) là một trong những lỗi phổ biến nhất. Chúng gây phiền toái cho người theo dõi bài viết, hay muốn tìm hiểu một nội dung nào đó trên website. Để crawl dữ liệu qua trình thu thập dữ liệu web Google là một cách thức tuyệt vời để rà soát các lỗi này, giúp tìm ra nhanh chóng các liên kết bị hỏng và sửa lại để đảm bảo website hoạt động ổn định.\\nSong, khi phát hiện các liên kết hỏng bạn có thể khắc phục bằng cách: xóa liên kết, thay thế liên kết hoặc báo cáo sự cố với chủ sở hữu trang web liên kết nếu đó là liên kết ngoài.\\nTìm nội dung trùng lặp trên hệ thống nội dung hiển thị\\nNội dung trùng lặp là nội dung giống hệt hoặc gần giống nhau. Khi sử dụng công cụ crawl google search results kiểm tra trang web có thể giúp bạn tìm ra [nội dung trùng lặp](https://cas-solution.vn/duplicate-content/).\\nNhững nội dung này có thể gây ra các vấn đề SEO nghiêm trọng, gây nhầm lẫn cho các công cụ tìm kiếm. Nó cũng có thể khiến phiên bản trang không đúng hiển thị trong kết quả tìm kiếm hoặc thậm chí bộ máy đọc của Google có thể nhầm lẫn trong việc bạn có đang sử dụng các hoạt động thao túng gian dối hay không.\\nSau khi xác định được nội dung cần chỉnh sửa, bạn có thể chỉnh sửa lại để phù hợp hơn với kết quả tìm kiếm.\\nCác yếu tố ảnh hưởng đến việc ra quyết định crawl data from website của Google\\nKhông có lịch trình cụ thể về thời điểm Google thực hiện crawl data from website của bạn. Điều này có thể ví như một cuộc ghé thăm ngẫu nhiên khi bộ máy của Google “lê la” khắp khu vực chúng hoạt động.\\nTuy nhiên, một số yếu tố có thể ảnh hưởng đến tần suất truy cập, rà soát của Google bao gồm:\\n- Kích thước trang web lớn, có nhiều trang, hệ thống nội dung, thông tin lớn và có lượng truy cập thường xuyên cao.\\n- Mức độ phổ biến của trang: Nếu trang web của bạn có số lượng liên kết inbound cao từ các nguồn uy tín, Google sẽ coi đây là tín hiệu của một tài nguyên chất lượng, có khả năng dẫn đến việc thu thập dữ liệu thường xuyên hơn.\\n[Tốc độ của website](https://cas-solution.vn/toc-do-website/): Một trang web tải nhanh sẽ dễ dàng hơn cho Googlebot thu thập dữ liệu.- Độ mới, sáng tạo của nội dung: Nếu bạn liên tục cập nhật nội dung mới, có liên quan trên trang web của mình thì Google có khả năng sẽ ghé thăm thường xuyên hơn để cập nhật chỉ mục.\\nMột số lỗi thường gặp khi “crawl data from website”\\nNgoài việc tìm hiểu crawl là gì, cách thức hoạt động, tầm quan trọng của việc quét dữ liệu thì trong bài viết này CAS Solution sẽ cùng bạn tìm hiểu về một số lỗi thường gặp khi trình thu thập dữ liệu của công cụ tìm kiếm không thể điều hướng qua các trang web theo cách thông thường. Một số lỗi phổ biến như:\\n- Bị chặn bởi\\n[robots.txt](https://cas-solution.vn/robots-txt/): Website hoặc một số trang con trên trang web có thể bị chặn bởi một lệnh trong tệp Robots.txt. Điều này ngăn chặn Googlebot hoặc các công cụ tìm kiếm khác thu thập thông tin các trang đó. - Lỗi Error 404: Trang trả về lỗi 404 không thể thu thập dữ liệu. Lỗi này có thể xảy ra khi các trang bị xóa hoặc URL của chúng được thay đổi mà không có hướng dẫn phù hợp.\\n- Lỗi máy chủ: Nếu máy chủ thường xuyên hoạt động liên tục hoặc phản hồi chậm, Googlebot có thể gặp sự cố khi thu thập trang web dữ liệu của bạn.\\n- Trang web quá chậm: Nếu trang web tải quá chậm, Googlebot có thể bỏ cuộc trước khi thu thập dữ liệu xong. Điều này chủ yếu áp xuất hiện khi trang web chậm một cách khó hiểu.\\n- Lạm dụng thẻ meta: Việc sử dụng thẻ meta noindex hoặc nofollow không chính xác cũng có thể ngăn chặn Google thu thập dữ liệu trang web của bạn.\\nLàm thế nào để tăng khả năng thu thập thông tin?\\nNói chung, để website hoạt động và tăng trưởng bền bỉ, người vận hành web cần phải làm cho crawl dữ liệu từ công cụ tìm kiếm càng nhiều càng tốt. Dưới đây là một số điều có thể thực hiện để tăng khả năng thu thập dữ liệu của trang web như:\\n- Tạo ra nội dung chất lượng cao được cập nhật thường xuyên\\n- Cải thiện tốc độ hoạt động của trang web nhanh nhất có thể\\n- Kiểm tra các liên kết nội bộ để điều hướng tốt hơn\\n- Sử dụng các cụm từ khóa trọng tâm, tối ưu hóa hình ảnh và văn bản thay thế\\n- Tạo và gửi sitemap.xml để đảm bảo rằng trình thu thập thông tin không bị nhầm lẫn, bỏ sót việc lập chỉ mục các trang quan trọng.\\n- Thiết kế trang web của bạn thân thiện với thiết bị di động\\nTóm lại, trình thu thập dữ liệu chính của Google, Googlebot, hoạt động theo các thuật toán phức tạp, nhưng bạn vẫn có thể “điều hướng” hành vi của nó để làm cho nó có lợi cho trang web của bạn. Bên cạnh đó, hầu hết các bước tối ưu hóa quy trình thu thập dữ liệu đều lặp lại các bước của SEO chuẩn mà chúng ta đều quen thuộc.\\nTrên đây, những thông tin giải đáp về crawl là gì và một số khía cạnh của việc crawl dữ liệu trên các công cụ tìm kiếm. Cảm ơn bạn đã dành thời gian tham khảo bài viết của CAS SOLUTION. Để được tư vấn về [dịch vụ chăm sóc website](https://cas-solution.vn/phat-trien-website/), vui lòng liên hệ tới hotline để được hỗ trợ nhanh nhất. Chúc bạn luôn thành công trong lĩnh vực kinh doanh của mình!. '}]\n"
          ]
        }
      ],
      "source": [
        "# from obsei.source.website_crawler_source import (\n",
        "#     TrafilaturaCrawlerConfig,\n",
        "#     TrafilaturaCrawlerSource,\n",
        "# )\n",
        "import pandas as pd\n",
        "#Single URL\n",
        "from trafilatura import extract, fetch_url\n",
        "source_config=TrafilaturaCrawlerConfig(\n",
        "    urls=[\"https://cas-solution.vn/crawl/\"],\n",
        "    include_comments = False,\n",
        "    # include_tables = False,\n",
        "    # no_fallback = False,\n",
        "    # include_images = False,\n",
        "    # include_formatting = False,\n",
        "    # deduplicate = False,\n",
        "    # no_ssl = False,\n",
        "    # is_feed = False,\n",
        "    #is_sitemap = True,\n",
        "    # include_links = False,\n",
        ")\n",
        "source=TrafilaturaCrawlerSource()\n",
        "urls=[\"https://cas-solution.vn/crawl/\"]\n",
        "source_response_list=source.lookup(source_config)\n",
        "print(source_response_list)\n",
        "#Extract information from data\n",
        "id = []\n",
        "hostname = []\n",
        "title = []\n",
        "raw_text = []\n",
        "rating = []\n",
        "platform = []\n",
        "for i in range(0, len(urls)):\n",
        "    print(i)\n",
        "    source_response_list[i] = dict(source_response_list[i])\n",
        "    print(source_response_list)\n",
        "    # id.append(source_response_list[i][\"meta\"][\"id\"])\n",
        "    # hostname.append(source_response_list[i][\"meta\"][\"hostname\"])\n",
        "    # title.append(source_response_list[i][\"meta\"][\"title\"])\n",
        "    # raw_text.append(source_response_list[i][\"meta\"][\"raw_text\"])\n",
        "data = pd.DataFrame(list(zip(id, hostname, title, raw_text)), columns=['id','author','title','content'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Atvtu7cNzbhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "0DBNcLOt7E_M",
        "outputId": "bfb71877-6a22-4741-eb6c-3dde7dedcab4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], Name: content, dtype: object)"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "data[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAWOjbFO7MAU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "for val in data[\"content\"].values:\n",
        "  text=str(val)\n",
        "  print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHsqAyiU7VqN"
      },
      "source": [
        "#Apply Model to summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_fJVnqr7Snl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "   device = torch.device(\"cuda\")\n",
        "else:\n",
        "   device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "-K4Ltg6J8F0B",
        "outputId": "115e7188-8778-4d70-ba47-c7c42818b8c3"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-49b83002a19e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m            \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nSummarized: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3341\u001b[0m         \"\"\"\n\u001b[1;32m   3342\u001b[0m         \u001b[0;31m# Convert inputs to python lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3343\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3345\u001b[0m         return self._decode(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_is_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_is_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m_is_tensorflow\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoordination_config_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/function_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mop_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_op__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mmessage_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menum_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mis_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       serialized_options=None, file=DESCRIPTOR),\n\u001b[0m\u001b[1;32m     43\u001b[0m     _descriptor.FieldDescriptor(\n\u001b[1;32m     44\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tensorflow.TensorShapeProto.Dim.name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[0;32m--> 560\u001b[0;31m       \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
          ]
        }
      ],
      "source": [
        "from langdetect import detect\n",
        "for index, row in data.iterrows():\n",
        "    text = row['content']\n",
        "    if detect(text) == \"vi\":\n",
        "        import pandas as pd\n",
        "        from transformers import AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "        import torch\n",
        "        tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "        preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
        "        tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)\n",
        "        summary_ids = model.generate(\n",
        "           tokenized_text,\n",
        "           max_length=512,\n",
        "           num_beams=4,\n",
        "           repetition_penalty=2.5,\n",
        "           length_penalty=1.0,\n",
        "           early_stopping=True\n",
        "    )\n",
        "        output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        print(\"\\n\\nSummarized: \\n\", output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf1T60Xe_w7I",
        "outputId": "413e9d9c-d993-43f7-dc9c-7a93a3347238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Summarized: \n",
            " Công ty Cổ phần Vận tải đường sắt Hà Nội cho biết, sau thời gian dài đình trệ do Covid-19, doanh nghiệp sẽ chạy lại đôi tàu khách SP3/SP4 phục vụ khách du lịch Sa Pa dịp cuối tuần, nếu khách đông sẽ cho chạy tàu hàng ngày.\n",
            "\n",
            "\n",
            "Summarized: \n",
            " Công ty Cổ phần Vận tải đường sắt Hà Nội cho biết, sau thời gian dài đình trệ do Covid-19, doanh nghiệp sẽ chạy lại đôi tàu khách SP3/SP4 phục vụ khách du lịch Sa Pa dịp cuối tuần, nếu khách đông sẽ cho chạy tàu hàng ngày.\n",
            "\n",
            "\n",
            "Summarized: \n",
            " Công ty Cổ phần Vận tải đường sắt Hà Nội cho biết, sau thời gian dài đình trệ do Covid-19, doanh nghiệp sẽ chạy lại đôi tàu khách SP3/SP4 phục vụ khách du lịch Sa Pa dịp cuối tuần, nếu khách đông sẽ cho chạy tàu hàng ngày.\n",
            "\n",
            "\n",
            "Summarized: \n",
            " Công ty Cổ phần Vận tải đường sắt Hà Nội cho biết, sau thời gian dài đình trệ do Covid-19, doanh nghiệp sẽ chạy lại đôi tàu khách SP3/SP4 phục vụ khách du lịch Sa Pa dịp cuối tuần, nếu khách đông sẽ cho chạy tàu hàng ngày.\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect\n",
        "for index, row in data.iterrows():\n",
        "    text = row['content']\n",
        "    if detect(text) == \"vi\":\n",
        "        import pandas as pd\n",
        "        from transformers import AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "        import torch\n",
        "        tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "        preprocess_text = text1.strip().replace(\"\\n\", \"\")\n",
        "        tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)\n",
        "        summary_ids = model.generate(\n",
        "           tokenized_text,\n",
        "           max_length=512,\n",
        "           num_beams=4,\n",
        "           repetition_penalty=2.5,\n",
        "           length_penalty=1.0,\n",
        "           early_stopping=True\n",
        "    )\n",
        "        output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        print(\"\\n\\nSummarized: \\n\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnMobanAHkEf"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhBlbeLQOv3g"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4TEgfkvANOT"
      },
      "outputs": [],
      "source": [
        "def summarization(data,tokenizer,model):\n",
        "    from transformers import AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "    import torch\n",
        "    from langdetect import detect\n",
        "    import pandas as pd\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    for val in data[\"content\"].values:\n",
        "        text=str(val)\n",
        "    for index, row in data.iterrows():\n",
        "        text = row['content']\n",
        "        if detect(text) == \"vi\":\n",
        "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = model.to(device)\n",
        "            preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
        "            tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)\n",
        "            summary_ids = model.generate(\n",
        "                tokenized_text,\n",
        "                max_length=512,\n",
        "                num_beams=8,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "            print(\"\\n\\nSummarized: \\n\", output)\n",
        "            data.loc[index, 'summary'] = output\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "AqChlTKsEPWC",
        "outputId": "01335e0a-54c1-4482-ad78-c11c9ca274c1"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b96c1dc11571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-04a055e638f0>\u001b[0m in \u001b[0;36msummarization\u001b[0;34m(data, tokenizer, model)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             )\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nSummarized: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3341\u001b[0m         \"\"\"\n\u001b[1;32m   3342\u001b[0m         \u001b[0;31m# Convert inputs to python lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3343\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3345\u001b[0m         return self._decode(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_is_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_is_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m_is_tensorflow\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoordination_config_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/function_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mop_def_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_op__def__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/attr_value_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mmessage_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menum_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mis_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       serialized_options=None, file=DESCRIPTOR),\n\u001b[0m\u001b[1;32m     43\u001b[0m     _descriptor.FieldDescriptor(\n\u001b[1;32m     44\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tensorflow.TensorShapeProto.Dim.name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[0;32m--> 560\u001b[0;31m       \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
          ]
        }
      ],
      "source": [
        "summarization(data=data,tokenizer=tokenizer,model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz1e3DgEE0XQ",
        "outputId": "37a993de-0133-43cf-e0b4-ab4c3a70f50e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ],
      "source": [
        "print(type(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPnUUrqUGyfI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "import pandas as pd\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "tokenizer.save_pretrained\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "def summarization(text):\n",
        "        preprocess_text = text.strip().replace(\"\\n\", \"\")\n",
        "        tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)\n",
        "        summary_ids = model.generate(\n",
        "                tokenized_text,\n",
        "                max_length=256,\n",
        "                num_beams=4,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True\n",
        "            )\n",
        "        output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J8T8QqiOgsr"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "import pandas as pd\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/summary/tokenizer_pretrained')\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "model.save_pretrained('/content/drive/MyDrive/summary/model_pretrained')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiPsetoJOeYx",
        "outputId": "03cea69e-0069-49f7-dcfc-feab6c265bbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "P6FgTz8kUwoR",
        "outputId": "a9bd627f-e576-4eee-ea6d-a90875663e17"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tỉnh Bà Rịa – Vũng Tàu sẽ chi 670 tỷ đồng, trong tổng số 1.333 tỷ đồng giải phóng mặt bằng đoạn 19,5 km cao tốc Biên Hòa – Vũng Tàu qua địa phận tỉnh này. Động thái của tỉnh đưa ra sau khi dự án tuyến đường được Thủ tướng chấp thuận đầu tư bằng ngân sách nhà nước.'"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summarization(text=\"\"\"Địa phương sẽ chi 670 tỷ đồng, trong tổng số 1.333 tỷ đồng giải phóng mặt bằng đoạn 19,5 km cao tốc Biên Hòa – Vũng Tàu qua địa phận tỉnh này.\n",
        "Nội dung được thông qua tại kỳ họp HĐNĐ tỉnh Bà Rịa – Vũng Tàu khóa VII, nhiệm kỳ 2021-2026, ngày 12/4.\n",
        "Động thái của tỉnh đưa ra sau khi dự án tuyến đường được Thủ tướng chấp thuận đầu tư bằng ngân sách nhà nước.\n",
        "Cao tốc Biên Hòa – Vũng Tàu (giai đoạn 1) dài 53,7 km, quy mô 4-6 làn xe, tổng mức đầu tư 17.837 tỷ đồng, dự kiến khởi công đầu năm sau và hoàn thành năm 2025.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIhbFO6KDUo7",
        "outputId": "1e7ecd6b-3dfd-4c8f-ad97-d2bc843346ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.13.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.4)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.2.0 selenium-4.13.0 trio-0.22.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "JKr1uK-YDY4_",
        "outputId": "64e130d9-c080-4723-ebd5-57e9e0ba0a68"
      },
      "outputs": [
        {
          "ename": "SessionNotCreatedException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-75ddbfe3c545>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Sử dụng hàm để crawl dữ liệu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://baivan.net/content/giai-bai-tap-doc-thu-gui-cac-hoc-sinh.html\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawl_questions_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# In ra kết quả\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-75ddbfe3c545>\u001b[0m in \u001b[0;36mcrawl_questions_answers\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcrawl_questions_answers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Khởi tạo trình duyệt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Chọn trình duyệt của bạn (Chrome, Firefox, ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             super().__init__(\n\u001b[0m\u001b[1;32m     57\u001b[0m                 command_executor=ChromiumRemoteConnection(\n\u001b[1;32m     58\u001b[0m                     \u001b[0mremote_server_addr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"capabilities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: Chrome failed to start: exited normally.\n  (session not created: DevToolsActivePort file doesn't exist)\n  (The process started from chrome location /root/.cache/selenium/chrome/linux64/117.0.5938.92/chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\nStacktrace:\n#0 0x5721b56fc6b3 <unknown>\n#1 0x5721b53d21e7 <unknown>\n#2 0x5721b5405526 <unknown>\n#3 0x5721b540169c <unknown>\n#4 0x5721b544423a <unknown>\n#5 0x5721b543ae93 <unknown>\n#6 0x5721b540d934 <unknown>\n#7 0x5721b540e71e <unknown>\n#8 0x5721b56c1cb8 <unknown>\n#9 0x5721b56c5bf0 <unknown>\n#10 0x5721b56d019c <unknown>\n#11 0x5721b56c6808 <unknown>\n#12 0x5721b569327f <unknown>\n#13 0x5721b56eae88 <unknown>\n#14 0x5721b56eb059 <unknown>\n#15 0x5721b56fb843 <unknown>\n#16 0x79b018334b43 <unknown>\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "\n",
        "def crawl_questions_answers(url):\n",
        "    # Khởi tạo trình duyệt\n",
        "    driver = webdriver.Chrome()  # Chọn trình duyệt của bạn (Chrome, Firefox, ...)\n",
        "    driver.get(url)\n",
        "\n",
        "    # Đợi cho trang web tải hoàn thành (có thể cần tùy chỉnh thời gian chờ)\n",
        "    # Thông qua implicit wait\n",
        "    driver.implicitly_wait(10)\n",
        "\n",
        "    # Lấy danh sách các câu hỏi và câu trả lời\n",
        "    questions = driver.find_elements_by_xpath('//div[@class=\"entry-content\"]/strong')\n",
        "    answers = driver.find_elements_by_xpath('//div[@class=\"entry-content\"]/p')\n",
        "\n",
        "    # Lưu kết quả vào một list\n",
        "    result = []\n",
        "    for question, answer in zip(questions, answers):\n",
        "        result.append({\n",
        "            \"question\": question.text,\n",
        "            \"answer\": answer.text\n",
        "        })\n",
        "\n",
        "    # Đóng trình duyệt\n",
        "    driver.quit()\n",
        "\n",
        "    return result\n",
        "\n",
        "# Sử dụng hàm để crawl dữ liệu\n",
        "url = \"https://baivan.net/content/giai-bai-tap-doc-thu-gui-cac-hoc-sinh.html\"\n",
        "data = crawl_questions_answers(url)\n",
        "\n",
        "# In ra kết quả\n",
        "for entry in data:\n",
        "    print(f\"Question: {entry['question']}\")\n",
        "    print(f\"Answer: {entry['answer']}\")\n",
        "    print(\"=\"*30)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4Ho0SFdEp0XW",
        "O8yTJCWUuSNM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}