import streamlit as st
from datetime import datetime

from db import EasyMongo
from llm_strings import LLMStrings, time_stamp
from utils import get_model_and_check_tokens, output_text, simulate_response, MAX_TOKENS_PER_MODEL

#-------------K·∫øt n·ªëi MongoDB (ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn)----------------
if "mongo_server" not in st.session_state:
    st.session_state.mongo_server = EasyMongo()
    st.session_state.mongo_server.init_ttl_index()  # Kh·ªüi t·∫°o TTL Index
    st.session_state.mongo_server.init_vectorstore_ttl_index()  # Kh·ªüi t·∫°o TTL Index cho vectorstore

mongo_server = st.session_state.mongo_server


# ------------------------ MODEL SELECTION ------------------------ #

# N·∫øu ch∆∞a c√≥ session_id trong session_state, kh·ªüi t·∫°o m·∫∑c ƒë·ªãnh
if "session_id" not in st.session_state:
    st.session_state.session_id = str(time_stamp())

if "messages" not in st.session_state:
    st.session_state.messages = []

# Danh s√°ch model h·ªó tr·ª£
MODEL_OPTIONS = ["gemini-2.0-flash", "gpt-3.5-turbo", "pixtral-12b-2409"]

# L·∫•y model ƒëang s·ª≠ d·ª•ng
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "gemini-2.0-flash"  # M·∫∑c ƒë·ªãnh

st.sidebar.title("Model Selection") 

# Ki·ªÉm tra token usage c·ªßa t·ª´ng model
gemini_usage = mongo_server.get_token_usage("gemini-2.0-flash")
gpt_usage = mongo_server.get_token_usage("gpt-3.5-turbo")
pixtral_usage = mongo_server.get_token_usage("pixtral-12b-2409")

# X√°c ƒë·ªãnh tr·∫°ng th√°i c·ªßa t·ª´ng model
gemini_disabled = gemini_usage >= MAX_TOKENS_PER_MODEL
gpt_disabled = gpt_usage >= MAX_TOKENS_PER_MODEL
pixtral_disabled = pixtral_usage >= MAX_TOKENS_PER_MODEL

# T·∫°o danh s√°ch options v·ªõi nh√£n t√πy ch·ªânh ƒë·ªÉ hi·ªÉn th·ªã tr·∫°ng th√°i
options_with_status = [
    f"gemini-2.0-flash{' (H·∫øt token)' if gemini_disabled else ''}",
    f"gpt-3.5-turbo{' (H·∫øt token)' if gpt_disabled else ''}",
    f"pixtral-12b-2409{' (H·∫øt token)' if pixtral_disabled else ''}"
]

# X√°c ƒë·ªãnh model m·∫∑c ƒë·ªãnh d·ª±a tr√™n token c√≤n l·∫°i
# if gemini_disabled and not gpt_disabled:
#     default_model = "gpt-3.5-turbo"
# elif gpt_disabled and not gemini_disabled:
#     default_model = "gemini-2.0-flash"
# else:
#     default_model = "gemini-2.0-flash" if not gemini_disabled else "gpt-3.5-turbo"

if not gemini_disabled:
    default_model = "gemini-2.0-flash"
elif not gpt_disabled:
    default_model = "gpt-3.5-turbo"
elif not pixtral_disabled:
    default_model = "pixtral-12b-2409"
else:
    default_model = None


if "selected_model" not in st.session_state or st.session_state.selected_model not in MODEL_OPTIONS:
    st.session_state.selected_model = default_model

# Radio ch·ªçn model tr√™n sidebar
selected_model_with_status = st.sidebar.radio(
    "Ch·ªçn Model:",
    options_with_status,
    index=options_with_status.index(
        f"{st.session_state.selected_model}{' (H·∫øt token)' if (st.session_state.selected_model == 'gemini-2.0-flash' and gemini_disabled) or (st.session_state.selected_model == 'gpt-3.5-turbo' and gpt_disabled) or (st.session_state.selected_model == 'pixtral-12b-2409' and pixtral_disabled) else ''}"
    ),
    disabled=(gemini_disabled and gpt_disabled and pixtral_disabled)
)

# L·∫•y model th·ª±c t·∫ø t·ª´ l·ª±a ch·ªçn (lo·∫°i b·ªè ph·∫ßn tr·∫°ng th√°i)
selected_model = selected_model_with_status.split(" (")[0]

# T·ª± ƒë·ªông chuy·ªÉn n·∫øu model ƒë∆∞·ª£c ch·ªçn h·∫øt token
if selected_model == "gemini-2.0-flash" and gemini_disabled:
    if not gpt_disabled:
        st.session_state.selected_model = "gpt-3.5-turbo"
    elif not pixtral_disabled:
        st.session_state.selected_model = "pixtral-12b-2409"
    st.rerun()
elif selected_model == "gpt-3.5-turbo" and gpt_disabled:
    if not gemini_disabled:
        st.session_state.selected_model = "gemini-2.0-flash"
    elif not pixtral_disabled:
        st.session_state.selected_model = "pixtral-12b-2409"
    st.rerun()
elif selected_model == "pixtral-12b-2409" and pixtral_disabled:
    if not gemini_disabled:
        st.session_state.selected_model = "gemini-2.0-flash"
    elif not gpt_disabled:
        st.session_state.selected_model = "gpt-3.5-turbo"
    st.rerun()
elif selected_model != st.session_state.selected_model:
    previous_session_id = st.session_state.session_id
    st.session_state.selected_model = selected_model
    st.session_state.session_id = previous_session_id
    st.rerun()

# Warning khi LLM Models h·∫øt token
if gemini_disabled and not gpt_disabled and not pixtral_disabled:
    st.sidebar.warning("‚ö†Ô∏è Model Gemini ƒë√£ h·∫øt token. ƒê√£ chuy·ªÉn sang model kh√°c!")
elif not gemini_disabled and gpt_disabled and not pixtral_disabled:
    st.sidebar.warning("‚ö†Ô∏è Model GPT ƒë√£ h·∫øt token. ƒê√£ chuy·ªÉn sang model kh√°c!")
elif not gemini_disabled and not gpt_disabled and pixtral_disabled:
    st.sidebar.warning("‚ö†Ô∏è Model Pixtral ƒë√£ h·∫øt token. ƒê√£ chuy·ªÉn sang model kh√°c!")
elif gemini_disabled and gpt_disabled and not pixtral_disabled:
    st.sidebar.warning("‚ö†Ô∏è Gemini v√† GPT h·∫øt token. ƒê√£ chuy·ªÉn sang Pixtral!")
elif gemini_disabled and gpt_disabled and pixtral_disabled:
    st.sidebar.warning("‚ö†Ô∏è T·∫•t c·∫£ model ƒë·ªÅu h·∫øt token. Vui l√≤ng th·ª≠ l·∫°i sau 24h!")
#--------------------------------------------------------------------------------# 


# -----------------------Giao di·ªán chat------------------------------------------#

# Giao di·ªán ·ª©ng d·ª•ng
st.title(LLMStrings.APP_TITLE)

# L·∫•y model ƒëang s·ª≠ d·ª•ng
model_name, model = get_model_and_check_tokens(mongo_server, preferred_model=st.session_state.selected_model)

if model_name:
    st.write(f"### Model: {model_name}")
else:
    st.write("### ‚ùå T·∫•t c·∫£ model ƒë·ªÅu h·∫øt token!")

#---------------------------------------------------------------------------------#


# ================================ SIDE BAR ======================================#
st.sidebar.title("Chat Sessions")

# L·∫•y danh s√°ch session t·ª´ MongoDB
chat_sessions = mongo_server.get_chat_sessions() or []

# ------------------------------ Caching d·ªØ li·ªáu -------------------------------- #
@st.cache_data
def get_cached_messages(session_id):
    return mongo_server.get_recent_messages(session_id) or []

# ------------------------ New Chat - T·∫°o session m·ªõi --------------------------- #

if st.sidebar.button("üÜï New Chat"):
    new_session_id = str(time_stamp())  # T·∫°o session_id m·ªõi
    st.session_state.session_id = new_session_id
    st.session_state.messages = []  # Reset tin nh·∫Øn tr√°nh b·ªã duplicate

    # Ch·ªâ th√™m session v√†o DB n·∫øu ch∆∞a t·ªìn t·∫°i
    if new_session_id not in chat_sessions:
        mongo_server.insert_messages(new_session_id, [
            {"session_id": new_session_id, "role": LLMStrings.AI_ROLE, "content": "Hello, how can I help you?", "timestamp": datetime.utcnow()}
        ])
        chat_sessions.append(new_session_id)

    # C·∫≠p nh·∫≠t UI ngay l·∫≠p t·ª©c
    st.rerun()

# ---------------------------- Ch·ªçn session t·ª´ l·ªãch s·ª≠ ---------------------------- #

if chat_sessions:
    selected_session = st.sidebar.radio(
        "History Chat",
        chat_sessions,
        index=chat_sessions.index(st.session_state.session_id) if st.session_state.session_id in chat_sessions else 0
    )

    if selected_session != st.session_state.session_id:
        st.session_state.session_id = selected_session
        st.session_state.messages = get_cached_messages(selected_session)  # D√πng caching ƒë·ªÉ gi·∫£m query
        st.rerun()

#-------------------------Th√™m ph·∫ßn upload file v√†o sidebar------------------------ #

st.sidebar.title("Upload File")
uploaded_file = st.sidebar.file_uploader("Upload a .txt, .pdf, or .docx file", type=["txt", "pdf", "docx"])

#================================================================================== #



#----------------------------------Ph·∫ßn CHATBOT------------------------------------ #
# Hi·ªÉn th·ªã l·ªùi ch√†o t·ª´ AI n·∫øu ch∆∞a c√≥ tin nh·∫Øn
if not st.session_state.messages:
    with st.chat_message(LLMStrings.AI_ROLE):
        st.write(LLMStrings.GREETINGS)

# Hi·ªÉn th·ªã l·ªãch s·ª≠ tin nh·∫Øn trong session
for message in st.session_state.messages:
    role = message["role"]
    if role not in [LLMStrings.USER_ROLE, LLMStrings.AI_ROLE]:
        role = LLMStrings.AI_ROLE
    with st.chat_message(role):
        st.markdown(message["content"])

# X·ª≠ l√Ω tin nh·∫Øn ƒë·∫ßu v√†o t·ª´ ng∆∞·ªùi d√πng
if prompt := st.chat_input(LLMStrings.INPUT_PLACEHOLDER):
    with st.chat_message(LLMStrings.USER_ROLE):
        st.markdown(prompt)
        st.session_state.messages.append({"role": LLMStrings.USER_ROLE, "content": prompt})

    with st.spinner(LLMStrings.WAIT_MESSAGE):
        with st.chat_message(LLMStrings.AI_ROLE):
            # Truy·ªÅn uploaded_file v√†o output_text n·∫øu c√≥
            response = output_text(prompt, mongo_server, session_id=st.session_state.session_id, uploaded_file=uploaded_file)
            st.session_state.messages.append({"role": LLMStrings.AI_ROLE, "content": response})
            simulate_response(response)

        mongo_server.insert_messages(st.session_state.session_id, [
            {"role": LLMStrings.USER_ROLE, "content": prompt, "timestamp": time_stamp()},
            {"role": LLMStrings.AI_ROLE, "content": response, "timestamp": time_stamp()}
        ])

#---------------------------------------------------------------------------------#
